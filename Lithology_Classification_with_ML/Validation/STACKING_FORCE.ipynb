{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "STACKING_FORCE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKzY0MX5EluI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "2dfada83-7539-41b5-a1b7-8f89189e7275"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numpy.random as nr\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
        "!pip install catboost\n",
        "from catboost import CatBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn import preprocessing\n",
        "import sklearn.model_selection as ms\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/39/128fff65072c8327371e3c594f3c826d29c85b21cb6485980353b168e0e4/catboost-0.24.2-cp36-none-manylinux1_x86_64.whl (66.1MB)\n",
            "\u001b[K     |████████████████████████████████| 66.2MB 47kB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.18.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (1.2.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-0.24.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGdm8oP7Y2jI"
      },
      "source": [
        "def fill_missing_values(data):\n",
        "    \n",
        "    '''\n",
        "    Function to input missing values based on the column object type\n",
        "    '''\n",
        "    \n",
        "    cols = list(data.columns)\n",
        "    for col in cols:\n",
        "        if data[col].dtype == 'int64' or data[col].dtype == 'float64':\n",
        "        \n",
        "            data[col] = data[col].fillna(data[col].mean())\n",
        "        \n",
        "        #elif data[col].dtype == 'O' or data[col].dtype == 'object':\n",
        "        #    data[col] = data[col].fillna(data[col].mode()[0])\n",
        "            \n",
        "        else:\n",
        "            data[col] = data[col].fillna(data[col].mode()[0])\n",
        "            \n",
        "    return data\n",
        " \n",
        "def one_hot_encoding(traindata, *args):\n",
        "    \n",
        "    for ii in args:\n",
        "        traindata = pd.get_dummies(traindata, prefix=[ii], columns=[ii])\n",
        "        \n",
        "    return traindata\n",
        " \n",
        "def drop_columns(traindata, *args):\n",
        "    \n",
        "    #labels = np.array(traindata[target])\n",
        "    \n",
        "    columns = []\n",
        "    for _ in args:\n",
        "        columns.append(_)\n",
        "        \n",
        "    traindata = traindata.drop(columns, axis=1)\n",
        "    #traindata = traindata.drop(target, axis=1)\n",
        "    #testdata = testdata.drop(columns, axis=1)\n",
        "        \n",
        "    return traindata\n",
        " \n",
        "def process(traindata):\n",
        "    \n",
        "    cols = list(traindata.columns)\n",
        "    for _ in cols:\n",
        "        traindata[_] = np.where(traindata[_] == np.inf, -999, traindata[_])\n",
        "        traindata[_] = np.where(traindata[_] == np.nan, -999, traindata[_])\n",
        "        traindata[_] = np.where(traindata[_] == -np.inf, -999, traindata[_])\n",
        "        \n",
        "    return traindata\n",
        " \n",
        "def show_evaluation(pred, true):\n",
        "  print(f'Default score: {score(true.values, pred)}')\n",
        "  print(f'Accuracy is: {accuracy_score(true, pred)}')\n",
        "  print(f'F1 is: {f1_score(pred, true.values, average=\"weighted\")}')\n",
        " \n",
        "def freq_encode(data, cols):\n",
        "    for i in cols:\n",
        "        encoding = data.groupby(i).size()\n",
        "        encoding = encoding/len(data)\n",
        "        data[i + '_enc'] = data[i].map(encoding)\n",
        "    return data\n",
        " \n",
        " \n",
        "def mean_target(data, cols):\n",
        "    kf = KFold(5)\n",
        "    a = pd.DataFrame()\n",
        "    for tr_ind, val_ind in kf.split(data):\n",
        "        X_tr, X_val= data.iloc[tr_ind].copy(), data.iloc[val_ind].copy()\n",
        "        for col in cols:\n",
        "            means = X_val[col].map(X_tr.groupby(col).FORCE_2020_LITHOFACIES_LITHOLOGY.mean())\n",
        "            X_val[col + '_mean_target'] = means + 0.0001\n",
        "        a = pd.concat((a, X_val))\n",
        "    #prior = FORCE_2020_LITHOFACIES_LITHOLOGY.mean()\n",
        "    #a.fillna(prior, inplace=True)\n",
        "    return a\n",
        " \n",
        "def make_submission(prediction, filename):\n",
        " \n",
        "  path = '/content/drive/My Drive/FORCE-Lithology-Prediction/'\n",
        " \n",
        "  test = pd.read_csv('/content/drive/My Drive/FORCE-Lithology-Prediction/Test.csv', sep=';')\n",
        "  #test_prediction = model.predict(testdata)\n",
        " \n",
        "  #test_prediction\n",
        "  category_to_lithology = {y:x for x,y in lithology_numbers.items()}\n",
        "  test_prediction_for_submission = np.vectorize(category_to_lithology.get)(prediction)\n",
        "  np.savetxt(path+filename+'.csv', test_prediction_for_submission, header='lithology', fmt='%i')\n",
        " \n",
        "# Feature windows concatenation function\n",
        "def augment_features_window(X, N_neig):\n",
        "    \n",
        "    # Parameters\n",
        "    N_row = X.shape[0]\n",
        "    N_feat = X.shape[1]\n",
        " \n",
        "    # Zero padding\n",
        "    X = np.vstack((np.zeros((N_neig, N_feat)), X, (np.zeros((N_neig, N_feat)))))\n",
        " \n",
        "    # Loop over windows\n",
        "    X_aug = np.zeros((N_row, N_feat*(2*N_neig+1)))\n",
        "    for r in np.arange(N_row)+N_neig:\n",
        "        this_row = []\n",
        "        for c in np.arange(-N_neig,N_neig+1):\n",
        "            this_row = np.hstack((this_row, X[r+c]))\n",
        "        X_aug[r-N_neig] = this_row\n",
        " \n",
        "    return X_aug\n",
        " \n",
        "# Feature gradient computation function\n",
        "def augment_features_gradient(X, depth):\n",
        "    \n",
        "    # Compute features gradient\n",
        "    d_diff = np.diff(depth).reshape((-1, 1))\n",
        "    d_diff[d_diff==0] = 0.001\n",
        "    X_diff = np.diff(X, axis=0)\n",
        "    X_grad = X_diff / d_diff\n",
        "        \n",
        "    # Compensate for last missing value\n",
        "    X_grad = np.concatenate((X_grad, np.zeros((1, X_grad.shape[1]))))\n",
        "    \n",
        "    return X_grad\n",
        " \n",
        "# Feature augmentation function\n",
        "def augment_features(X, well, depth, N_neig=1):\n",
        "    \n",
        "    # Augment features\n",
        "    X_aug = np.zeros((X.shape[0], X.shape[1]*(N_neig*2+2)))\n",
        "    for w in np.unique(well):\n",
        "        w_idx = np.where(well == w)[0]\n",
        "        X_aug_win = augment_features_window(X[w_idx, :], N_neig)\n",
        "        X_aug_grad = augment_features_gradient(X[w_idx, :], depth[w_idx])\n",
        "        X_aug[w_idx, :] = np.concatenate((X_aug_win, X_aug_grad), axis=1)\n",
        "    \n",
        "    # Find padded rows\n",
        "    padded_rows = np.unique(np.where(X_aug[:, 0:7] == np.zeros((1, 7)))[0])\n",
        "    \n",
        "    return X_aug, padded_rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuul_30fY2-y"
      },
      "source": [
        "A = np.load('/content/drive/My Drive/FORCE-Lithology-Prediction/penalty_matrix.npy')\n",
        " \n",
        "def score(y_true, y_pred):\n",
        "    S = 0.0\n",
        "    y_true = y_true.astype(int)\n",
        "    y_pred = y_pred.astype(int)\n",
        "    for i in range(0, y_true.shape[0]):\n",
        "        S -= A[y_true[i], y_pred[i]]\n",
        "    return S/y_true.shape[0]\n",
        " \n",
        "def evaluate(model):\n",
        "    feat_imp = pd.Series(model.feature_importances_).sort_values(ascending=False)\n",
        "    plt.figure(figsize=(24,8))\n",
        "    feat_imp.plot(kind='bar', title=f'Feature Importances {len(model.feature_importances_)}')\n",
        "    plt.ylabel('Feature Importance Score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfI7IeJG6Z6E"
      },
      "source": [
        "#importing miles\n",
        "train = pd.read_csv('/content/drive/My Drive/FORCE-Lithology-Prediction/train1.csv')\n",
        "test = pd.read_csv('/content/drive/My Drive/FORCE-Lithology-Prediction/Test.csv', sep=';')\n",
        "valid1 = pd.read_csv('/content/drive/My Drive/FORCE-Lithology-Prediction/valid4.csv')\n",
        "valid2 = pd.read_csv('/content/drive/My Drive/FORCE-Lithology-Prediction/valid5.csv')\n",
        "valid3 = pd.concat((valid1, valid2)).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfak56aaqID2"
      },
      "source": [
        "lithology = train['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
        "valid1_lithology = valid1['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
        "valid2_lithology = valid2['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
        "valid3_lithology = valid3['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
        " \n",
        "lithology_numbers = {30000: 0,\n",
        "                 65030: 1,\n",
        "                 65000: 2,\n",
        "                 80000: 3,\n",
        "                 74000: 4,\n",
        "                 70000: 5,\n",
        "                 70032: 6,\n",
        "                 88000: 7,\n",
        "                 86000: 8,\n",
        "                 99000: 9,\n",
        "                 90000: 10,\n",
        "                 93000: 11}\n",
        " \n",
        "lithology = lithology.map(lithology_numbers)\n",
        "valid1_lithology = valid1_lithology.map(lithology_numbers)\n",
        "valid2_lithology = valid2_lithology.map(lithology_numbers)\n",
        "valid3_lithology = valid3_lithology.map(lithology_numbers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DPPLupbavaU"
      },
      "source": [
        "train_well = train.WELL.values\n",
        "train_depth = train.DEPTH_MD.values\n",
        " \n",
        "valid1_well = valid1.WELL.values\n",
        "valid1_depth = valid1.DEPTH_MD.values\n",
        " \n",
        "valid2_well = valid2.WELL.values\n",
        "valid2_depth = valid2.DEPTH_MD.values\n",
        " \n",
        "valid3_well = valid3.WELL.values\n",
        "valid3_depth = valid3.DEPTH_MD.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt79NastY5VW"
      },
      "source": [
        "ntrain = train.shape[0]\n",
        "ntest = test.shape[0]\n",
        "nvalid1 = valid1.shape[0]\n",
        "nvalid2 = valid2.shape[0]\n",
        "nvalid3 = valid3.shape[0]\n",
        "target = train.FORCE_2020_LITHOFACIES_LITHOLOGY.copy()\n",
        "df = pd.concat((train, test, valid1, valid2, valid3)).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zMlY-7RZDBq"
      },
      "source": [
        "lithology = train['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
        "valid1_lithology = valid1['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
        "valid2_lithology = valid2['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
        "valid3_lithology = valid3['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
        " \n",
        "lithology_numbers = {30000: 0,\n",
        "                 65030: 0,\n",
        "                 65000: 0,\n",
        "                 80000: 0,\n",
        "                 74000: 0,\n",
        "                 70000: 0,\n",
        "                 70032: 0,\n",
        "                 88000: 0,\n",
        "                 86000: 0,\n",
        "                 99000: 0,\n",
        "                 90000: 0,\n",
        "                 93000: 1}\n",
        " \n",
        "lithology = lithology.map(lithology_numbers)\n",
        "valid1_lithology = valid1_lithology.map(lithology_numbers)\n",
        "valid2_lithology = valid2_lithology.map(lithology_numbers)\n",
        "valid3_lithology = valid3_lithology.map(lithology_numbers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlErqqCMVVIa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d927e167-fe0b-4905-8055-5bbe972eaa7b"
      },
      "source": [
        "print(df.shape)\n",
        "cols = ['FORCE_2020_LITHOFACIES_CONFIDENCE', 'SGR', 'DTS', 'RXO', 'ROPA']\n",
        "df = drop_columns(df, *cols)\n",
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1512843, 29)\n",
            "(1512843, 24)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRsjiPZqa49_"
      },
      "source": [
        "df['GROUP_encoded'] = df['GROUP'].astype('category')\n",
        "df['GROUP_encoded'] = df['GROUP_encoded'].cat.codes \n",
        "df['FORMATION_encoded'] = df['FORMATION'].astype('category')\n",
        "df['FORMATION_encoded'] = df['FORMATION_encoded'].cat.codes\n",
        "df['WELL_encoded'] = df['WELL'].astype('category')\n",
        "df['WELL_encoded'] = df['WELL_encoded'].cat.codes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV6wikmIa7Av",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "18f30bb2-0744-4372-b30a-1ae7048af06e"
      },
      "source": [
        "df = df.drop(['WELL', 'GROUP', 'FORMATION'], axis=1)\n",
        "df.shape\n",
        " \n",
        "df = df.fillna(-999)\n",
        "df = process(df)\n",
        "data = df.copy()\n",
        " \n",
        "train2 = data[:ntrain].copy()\n",
        "target = train2.FORCE_2020_LITHOFACIES_LITHOLOGY.copy()\n",
        "validation1_target = valid1.FORCE_2020_LITHOFACIES_LITHOLOGY.copy()\n",
        "validation2_target = valid2.FORCE_2020_LITHOFACIES_LITHOLOGY.copy()\n",
        "train2.drop(['FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)\n",
        " \n",
        "test2 = data[ntrain:(ntest+ntrain)].copy()\n",
        "test2.drop(['FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)\n",
        "test2 = test2.reset_index(drop=True)\n",
        " \n",
        "validation1 = data[(ntest+ntrain):(ntest+ntrain+nvalid1)].copy()\n",
        "validation1.drop(['FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)\n",
        "validation1 = validation1.reset_index(drop=True)\n",
        " \n",
        "validation2 = data[(ntrain+ntest+nvalid1): (ntrain+ntest+nvalid1+nvalid2)].copy()\n",
        "validation2.drop(['FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)\n",
        "validation2 = validation2.reset_index(drop=True)\n",
        " \n",
        " \n",
        "validation3 = data[(ntrain+ntest+nvalid1+nvalid2):].copy()\n",
        "validation3.drop(['FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)\n",
        "validation3 = validation3.reset_index(drop=True)\n",
        " \n",
        "print(train2.shape, test2.shape, validation1.shape, valid1.shape, validation2.shape, validation3.shape, valid2.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(964965, 23) (136786, 23) (114079, 23) (114079, 29) (91467, 23) (205546, 23) (91467, 29)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTL3T6Dva_X3"
      },
      "source": [
        "traindata = train2\n",
        "testdata = test2\n",
        "from sklearn.preprocessing import StandardScaler\n",
        " \n",
        "scaler = StandardScaler().fit(traindata)\n",
        "def scale_data(data):\n",
        "  \n",
        "  data = scaler.transform(data)\n",
        "  #testdata = scaler.transform(testdata)\n",
        "  data = pd.DataFrame(data, columns=testdata.columns)\n",
        " \n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydXv5V_jbEP9"
      },
      "source": [
        "#traindata, padded_rows = augment_features(traindata.values, train_well, train_depth)\n",
        "#validation1, padded_rows1 = augment_features(validation1.values, valid1_well, valid1_depth)\n",
        "#validation2, padded_rows2 = augment_features(validation2.values, valid2_well, valid2_depth)\n",
        "#validation3, padded_rows3 = augment_features(validation3.values, valid3_well, valid3_depth)\n",
        " \n",
        "#traindata.shape, validation1.shape, validation3.shape, validation2.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjYtAoM3bD7o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2ac32639-fcf9-4da0-b6b2-bd2b885920f9"
      },
      "source": [
        "validation, traindata, valid_target, lithology = ms.train_test_split(pd.DataFrame(traindata), lithology, random_state=405, test_size=0.9, stratify=lithology)\n",
        "print(validation.shape, valid_target.shape, traindata.shape, lithology.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(96496, 23) (96496,) (868469, 23) (868469,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAuYJnm4Q0ZY"
      },
      "source": [
        "def sort_data(data):\n",
        "    a = data.sort_index(inplace=False)\n",
        "    return a\n",
        " \n",
        "validation=sort_data(pd.DataFrame(validation))\n",
        "traindata=sort_data(pd.DataFrame(traindata))\n",
        "#validation.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0s24b_3a5-C"
      },
      "source": [
        "valid_target = sort_data(pd.DataFrame(valid_target))\n",
        "lithology = sort_data(pd.DataFrame(lithology))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoHq2Qa5bHuL"
      },
      "source": [
        "class Model():\n",
        "    \n",
        "    def __init__(self, train, test, validation1, validation2, validation3, train_label, valid_label1, valid_label2, valid_label3):\n",
        "        \n",
        "        \n",
        "        self.train = train\n",
        "        self.test = test\n",
        "        self.validation1 = validation1\n",
        "        self.validation2 = validation2\n",
        "        self.validation3 = validation3\n",
        "        self.train_label = train_label\n",
        "        self.valid_label1 = valid_label1\n",
        "        self.valid_label2 = valid_label2\n",
        "        self.valid_label3 = valid_label3\n",
        "        \n",
        "    def __call__(self, plot = True):\n",
        "        return self.fit(plot)\n",
        "    \n",
        "    def fit(self, plot):\n",
        "      \n",
        "      def show_evaluation(pred, true):\n",
        "        \n",
        "        print(f'Default score: {score(true.values, pred)}')\n",
        "        print(f'Accuracy is: {accuracy_score(true, pred)}')\n",
        "        print(f'F1 is: {f1_score(pred, true.values, average=\"weighted\")}')\n",
        " \n",
        "      split = 5\n",
        "      kf = StratifiedKFold(n_splits=split, shuffle=False)\n",
        "  \n",
        "      pred = np.zeros((len(self.test), 2))\n",
        "      val1 = np.zeros((len(self.validation1), 2))\n",
        "      val2 = np.zeros((len(self.validation2), 2))\n",
        "      val3 = np.zeros((len(self.validation3), 2))\n",
        " \n",
        "      #model = CatBoostClassifier(n_estimators=500000, random_state=2020, learning_rate=0.033,\n",
        "                                 #use_best_model=True, max_depth=8, reg_lambda=1500,\n",
        "                                 #eval_metric='MultiClass', task_type='GPU', verbose=100)\n",
        " \n",
        "      model = XGBClassifier(n_estimators=1000, max_depth=10, reg_lambda=500,\n",
        "                            objective='binary:logistic', learning_rate=0.1, random_state=250,\n",
        "                            subsample=0.9, col_sample_bytree=0.9, tree_method='gpu_hist',\n",
        "                            eval_metric='logloss', verbose=2020)\n",
        "      \n",
        "      #model = LGBMClassifier(n_estimators=50000, max_depth=10, reg_lambda=1200,\n",
        "                            #objective='multiclass', learning_rate=0.033,\n",
        "                            #eval_metric='multi_logloss', subsample=0.9, col_sample_bytree=0.9)\n",
        "      \n",
        "      #model = RandomForestClassifier(n_estimators=2, class_weight='balanced')\n",
        "      #model = ExtraTreesClassifier(n_estimators=100, class_weight='balanced', verbose=2)\n",
        "      i = 1\n",
        "      for (train_index, test_index) in kf.split(pd.DataFrame(traindata), pd.DataFrame(lithology)):\n",
        "        X_train,X_test = pd.DataFrame(traindata).iloc[train_index], pd.DataFrame(traindata).iloc[test_index]\n",
        "        Y_train,Y_test = pd.DataFrame(lithology).iloc[train_index],pd.DataFrame(lithology).iloc[test_index]\n",
        "    \n",
        "        \n",
        "        model.fit(X_train, Y_train, early_stopping_rounds=50, eval_set=[(X_test, Y_test)], verbose=20)\n",
        "        #model.fit(X_train, Y_train)\n",
        "        prediction1 = model.predict(pd.DataFrame(self.validation1))\n",
        "        prediction = model.predict(pd.DataFrame(self.validation2))\n",
        "        print(show_evaluation(prediction1, self.valid_label1))\n",
        "        print(show_evaluation(prediction, self.valid_label2))\n",
        " \n",
        "        print(f'-----------------------FOLD {i}---------------------')\n",
        "        i+=1\n",
        " \n",
        "        pred += model.predict_proba(pd.DataFrame(self.test))\n",
        "        val1 += model.predict_proba(pd.DataFrame(self.validation1))\n",
        "        val2 += model.predict_proba(pd.DataFrame(self.validation2))\n",
        "        val3 += model.predict_proba(pd.DataFrame(self.validation3))\n",
        "      \n",
        "      pred = pd.DataFrame(pred/split)\n",
        "      val1 = pd.DataFrame(val1/split)\n",
        "      val2 = pd.DataFrame(val2/split)\n",
        "      val3 = pd.DataFrame(val3/split)\n",
        "\n",
        "      pred = np.array(pred)\n",
        "      val1 = np.array(val1)\n",
        "      val2 = np.array(val2)\n",
        "      val3 = np.array(val3)\n",
        "    \n",
        "      #pred = np.array(pd.DataFrame(pred).idxmax(axis=1))\n",
        "      #val1 = np.array(pd.DataFrame(val1).idxmax(axis=1))\n",
        "      #val2 = np.array(pd.DataFrame(val2).idxmax(axis=1))\n",
        "      #val3 = np.array(pd.DataFrame(val3).idxmax(axis=1))\n",
        " \n",
        "      pred11 = np.array(pd.DataFrame(pred).idxmax(axis=1))\n",
        "      val11 = np.array(pd.DataFrame(val1).idxmax(axis=1))\n",
        "      val22 = np.array(pd.DataFrame(val2).idxmax(axis=1))\n",
        "      val33 = np.array(pd.DataFrame(val3).idxmax(axis=1))\n",
        " \n",
        "      print('---------------CROSS VALIDATION COMPLETE')\n",
        "      print('----------------TEST EVALUATION------------------')\n",
        " \n",
        "      print('----------Valid 1-------------')\n",
        "      print(show_evaluation(val11, self.valid_label1))\n",
        "      print('----------Valid 2-------------')\n",
        "      print(show_evaluation(val22, self.valid_label2))\n",
        "      print('----------Valid 3-------------')\n",
        "      print(show_evaluation(val33, self.valid_label3))\n",
        "                  \n",
        "      if plot: self.plot_feat_imp(model)\n",
        "      return pred[:,1], val1[:,1], val2[:,1], val3[:,1], model\n",
        "              \n",
        "              \n",
        "    def plot_feat_imp(self, model):\n",
        "        feat_imp = pd.Series(model.feature_importances_).sort_values(ascending=False)\n",
        "        plt.figure(figsize=(12,8))\n",
        "        feat_imp.plot(kind='bar', title='Feature Importances')\n",
        "        plt.ylabel('Feature Importance Score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU0398F_Omg5"
      },
      "source": [
        "#func1_= Model(traindata, validation, validation1, validation2, validation3, lithology, valid1_lithology, valid2_lithology, valid3_lithology)\n",
        "#pred1, val1, open_test1, open_test1a, model1 = func1_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L6Vhbw-T3R2"
      },
      "source": [
        "#func2_= Model(traindata, validation, validation1, validation2, validation3, lithology, valid1_lithology, valid2_lithology, valid3_lithology)\n",
        "#pred2, val2, open_test2, open_test22, model2 = func2_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn8Zfr7hbHdm"
      },
      "source": [
        "#func3_= Model(traindata, validation, validation1, validation2, validation3, lithology, valid1_lithology, valid2_lithology, valid3_lithology)\n",
        "#pred3, val3, open_test3, open_test33, model3 = func3_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHj_jt0DLvRt"
      },
      "source": [
        "#func4_= Model(traindata, validation, validation1, validation2, validation3, lithology, valid1_lithology, valid2_lithology, valid3_lithology)\n",
        "#pred4, val4, open_test4, open_test44, model4 = func4_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMWhq2lhL-Mi"
      },
      "source": [
        "#func5_= Model(traindata, validation, validation1, validation2, validation3, lithology, valid1_lithology, valid2_lithology, valid3_lithology)\n",
        "#pred5, val5, open_test5, open_test55, model5 = func5_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jHY573Hkrvh"
      },
      "source": [
        "#func6_= Model(traindata, validation, validation1, validation2, validation3, lithology, valid1_lithology, valid2_lithology, valid3_lithology)\n",
        "#pred6, val6, open_test6, open_test66, model6 = func6_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXzhMC9nTzCc"
      },
      "source": [
        "#func7_= Model(traindata, validation, validation1, validation2, validation3, lithology, valid1_lithology, valid2_lithology, valid3_lithology)\n",
        "#pred7, val7, open_test7, open_test77, model7 = func7_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqb8u6CMTzZ_"
      },
      "source": [
        "#func8_= Model(traindata, validation, validation1, validation2, validation3, lithology, valid1_lithology, valid2_lithology, valid3_lithology)\n",
        "#pred8, val8, open_test8, open_test88, model8 = func8_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHC_rmRQTzq5"
      },
      "source": [
        "#func9_= Model(traindata, validation, validation1, validation2, validation3, lithology, valid1_lithology, valid2_lithology, valid3_lithology)\n",
        "#pred9, val9, open_test9, open_test99, model9 = func9_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjV1O-Y5Tz-2"
      },
      "source": [
        "#func10_= Model(traindata, validation, validation1, validation2, validation3, lithology, valid1_lithology, valid2_lithology, valid3_lithology)\n",
        "#pred10, val10, open_test10, open_test110, model10 = func10_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSAiyUg3BCyw"
      },
      "source": [
        "#func11_= Model(traindata, validation, validation1, validation2, validation3, lithology, valid1_lithology, valid2_lithology, valid3_lithology)\n",
        "#pred11, val11, open_test11, open_test111, model11 = func11_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DePLoSw6BDfm"
      },
      "source": [
        "#func12_= Model(traindata, validation, validation1, validation2, validation3, lithology, valid1_lithology, valid2_lithology, valid3_lithology)\n",
        "#pred12, val12, open_test12, open_test112, model12 = func12_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW6_sdL7Mbpn"
      },
      "source": [
        "stack = np.column_stack((pred1, pred2, pred3, pred4, pred5, pred6, pred7, pred8, pred9, pred10, pred11, pred12))\n",
        "stack_p = np.column_stack((val1, val2, val3, val4, val5, val6, val7, val8, val9, val10, val11, val12))\n",
        "stack_open = np.column_stack((open_test1, open_test2, open_test3, open_test4, open_test5, open_test6, open_test7, open_test8, open_test9, open_test10, open_test11, open_test12))\n",
        "stack_open1 = np.column_stack((open_test1a, open_test22, open_test33, open_test44, open_test55, open_test66, open_test77, open_test88, open_test99, open_test110, open_test111, open_test112))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeXL3fyvLaH_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ea63edb1-5bfa-4b44-9f81-7c83d16dd82f"
      },
      "source": [
        "pd.DataFrame(stack_p).tail(50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>114029</th>\n",
              "      <td>0.374518</td>\n",
              "      <td>0.465062</td>\n",
              "      <td>0.405746</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092162</td>\n",
              "      <td>0.156459</td>\n",
              "      <td>0.091079</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090212</td>\n",
              "      <td>0.002093</td>\n",
              "      <td>0.005089</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114030</th>\n",
              "      <td>0.388508</td>\n",
              "      <td>0.465062</td>\n",
              "      <td>0.392066</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092162</td>\n",
              "      <td>0.160020</td>\n",
              "      <td>0.091079</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090211</td>\n",
              "      <td>0.002094</td>\n",
              "      <td>0.005466</td>\n",
              "      <td>0.066117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114031</th>\n",
              "      <td>0.405816</td>\n",
              "      <td>0.465062</td>\n",
              "      <td>0.351830</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092032</td>\n",
              "      <td>0.162112</td>\n",
              "      <td>0.091079</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090225</td>\n",
              "      <td>0.002093</td>\n",
              "      <td>0.005339</td>\n",
              "      <td>0.066118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114032</th>\n",
              "      <td>0.537770</td>\n",
              "      <td>0.459445</td>\n",
              "      <td>0.329910</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092050</td>\n",
              "      <td>0.172630</td>\n",
              "      <td>0.091080</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090229</td>\n",
              "      <td>0.002101</td>\n",
              "      <td>0.005694</td>\n",
              "      <td>0.066118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114033</th>\n",
              "      <td>0.687965</td>\n",
              "      <td>0.453513</td>\n",
              "      <td>0.262351</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092204</td>\n",
              "      <td>0.174366</td>\n",
              "      <td>0.091080</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090232</td>\n",
              "      <td>0.002101</td>\n",
              "      <td>0.005809</td>\n",
              "      <td>0.066118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114034</th>\n",
              "      <td>0.703408</td>\n",
              "      <td>0.453513</td>\n",
              "      <td>0.260091</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092402</td>\n",
              "      <td>0.179080</td>\n",
              "      <td>0.091183</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090241</td>\n",
              "      <td>0.002103</td>\n",
              "      <td>0.005768</td>\n",
              "      <td>0.066118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114035</th>\n",
              "      <td>0.714621</td>\n",
              "      <td>0.453513</td>\n",
              "      <td>0.258536</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092624</td>\n",
              "      <td>0.209170</td>\n",
              "      <td>0.091294</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090256</td>\n",
              "      <td>0.002105</td>\n",
              "      <td>0.005918</td>\n",
              "      <td>0.066118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114036</th>\n",
              "      <td>0.721492</td>\n",
              "      <td>0.453513</td>\n",
              "      <td>0.257109</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092839</td>\n",
              "      <td>0.233774</td>\n",
              "      <td>0.091388</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090262</td>\n",
              "      <td>0.002106</td>\n",
              "      <td>0.005733</td>\n",
              "      <td>0.066118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114037</th>\n",
              "      <td>0.717253</td>\n",
              "      <td>0.453513</td>\n",
              "      <td>0.258516</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.093353</td>\n",
              "      <td>0.209170</td>\n",
              "      <td>0.091295</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090246</td>\n",
              "      <td>0.002106</td>\n",
              "      <td>0.005678</td>\n",
              "      <td>0.066118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114038</th>\n",
              "      <td>0.708699</td>\n",
              "      <td>0.453513</td>\n",
              "      <td>0.260067</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.091993</td>\n",
              "      <td>0.176170</td>\n",
              "      <td>0.091184</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090224</td>\n",
              "      <td>0.002104</td>\n",
              "      <td>0.005549</td>\n",
              "      <td>0.066118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114039</th>\n",
              "      <td>0.701997</td>\n",
              "      <td>0.453513</td>\n",
              "      <td>0.261541</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.091815</td>\n",
              "      <td>0.173844</td>\n",
              "      <td>0.091080</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090174</td>\n",
              "      <td>0.002102</td>\n",
              "      <td>0.005244</td>\n",
              "      <td>0.066117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114040</th>\n",
              "      <td>0.660874</td>\n",
              "      <td>0.453513</td>\n",
              "      <td>0.288999</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.091585</td>\n",
              "      <td>0.171312</td>\n",
              "      <td>0.091080</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090168</td>\n",
              "      <td>0.002102</td>\n",
              "      <td>0.004910</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114041</th>\n",
              "      <td>0.596478</td>\n",
              "      <td>0.454251</td>\n",
              "      <td>0.309315</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.091419</td>\n",
              "      <td>0.167927</td>\n",
              "      <td>0.091080</td>\n",
              "      <td>0.001539</td>\n",
              "      <td>0.090168</td>\n",
              "      <td>0.002102</td>\n",
              "      <td>0.004942</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114042</th>\n",
              "      <td>0.589309</td>\n",
              "      <td>0.454251</td>\n",
              "      <td>0.309898</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.091419</td>\n",
              "      <td>0.167927</td>\n",
              "      <td>0.091080</td>\n",
              "      <td>0.001539</td>\n",
              "      <td>0.090168</td>\n",
              "      <td>0.002102</td>\n",
              "      <td>0.004957</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114043</th>\n",
              "      <td>0.654395</td>\n",
              "      <td>0.453513</td>\n",
              "      <td>0.288317</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.091436</td>\n",
              "      <td>0.169336</td>\n",
              "      <td>0.091080</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090170</td>\n",
              "      <td>0.002102</td>\n",
              "      <td>0.005046</td>\n",
              "      <td>0.066117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114044</th>\n",
              "      <td>0.697266</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.260309</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.091619</td>\n",
              "      <td>0.171083</td>\n",
              "      <td>0.091080</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090193</td>\n",
              "      <td>0.002102</td>\n",
              "      <td>0.005051</td>\n",
              "      <td>0.066118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114045</th>\n",
              "      <td>0.708183</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.259032</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.091876</td>\n",
              "      <td>0.173418</td>\n",
              "      <td>0.091184</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090205</td>\n",
              "      <td>0.002103</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>0.066118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114046</th>\n",
              "      <td>0.716434</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.258161</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.091967</td>\n",
              "      <td>0.189596</td>\n",
              "      <td>0.091276</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090214</td>\n",
              "      <td>0.002105</td>\n",
              "      <td>0.004852</td>\n",
              "      <td>0.066118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114047</th>\n",
              "      <td>0.720567</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.257318</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092350</td>\n",
              "      <td>0.204076</td>\n",
              "      <td>0.091348</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090213</td>\n",
              "      <td>0.002105</td>\n",
              "      <td>0.004902</td>\n",
              "      <td>0.066118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114048</th>\n",
              "      <td>0.719337</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.258288</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092318</td>\n",
              "      <td>0.182152</td>\n",
              "      <td>0.091269</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090216</td>\n",
              "      <td>0.002104</td>\n",
              "      <td>0.004856</td>\n",
              "      <td>0.066117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114049</th>\n",
              "      <td>0.716374</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.259108</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092384</td>\n",
              "      <td>0.171920</td>\n",
              "      <td>0.091139</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090217</td>\n",
              "      <td>0.002103</td>\n",
              "      <td>0.004694</td>\n",
              "      <td>0.066111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114050</th>\n",
              "      <td>0.702788</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.261113</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092263</td>\n",
              "      <td>0.173281</td>\n",
              "      <td>0.091080</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090211</td>\n",
              "      <td>0.002102</td>\n",
              "      <td>0.004501</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114051</th>\n",
              "      <td>0.664042</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.296730</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092160</td>\n",
              "      <td>0.171312</td>\n",
              "      <td>0.091080</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090209</td>\n",
              "      <td>0.002102</td>\n",
              "      <td>0.004459</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114052</th>\n",
              "      <td>0.613152</td>\n",
              "      <td>0.456333</td>\n",
              "      <td>0.312023</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092167</td>\n",
              "      <td>0.170044</td>\n",
              "      <td>0.091080</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090209</td>\n",
              "      <td>0.002102</td>\n",
              "      <td>0.004429</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114053</th>\n",
              "      <td>0.559720</td>\n",
              "      <td>0.461605</td>\n",
              "      <td>0.322084</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092104</td>\n",
              "      <td>0.163622</td>\n",
              "      <td>0.091080</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090209</td>\n",
              "      <td>0.002102</td>\n",
              "      <td>0.004425</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114054</th>\n",
              "      <td>0.524940</td>\n",
              "      <td>0.461605</td>\n",
              "      <td>0.326428</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092104</td>\n",
              "      <td>0.163823</td>\n",
              "      <td>0.091080</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090209</td>\n",
              "      <td>0.002101</td>\n",
              "      <td>0.004436</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114055</th>\n",
              "      <td>0.551177</td>\n",
              "      <td>0.461605</td>\n",
              "      <td>0.320822</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092104</td>\n",
              "      <td>0.163622</td>\n",
              "      <td>0.091080</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090209</td>\n",
              "      <td>0.002102</td>\n",
              "      <td>0.004219</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114056</th>\n",
              "      <td>0.616322</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.305963</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092173</td>\n",
              "      <td>0.170506</td>\n",
              "      <td>0.091080</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090209</td>\n",
              "      <td>0.002102</td>\n",
              "      <td>0.003897</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114057</th>\n",
              "      <td>0.685683</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.260517</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092263</td>\n",
              "      <td>0.172243</td>\n",
              "      <td>0.091080</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090211</td>\n",
              "      <td>0.002102</td>\n",
              "      <td>0.003845</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114058</th>\n",
              "      <td>0.685809</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.258795</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092345</td>\n",
              "      <td>0.173787</td>\n",
              "      <td>0.091182</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090220</td>\n",
              "      <td>0.002104</td>\n",
              "      <td>0.003818</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114059</th>\n",
              "      <td>0.685725</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.258384</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092370</td>\n",
              "      <td>0.175780</td>\n",
              "      <td>0.091216</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090220</td>\n",
              "      <td>0.002104</td>\n",
              "      <td>0.004304</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114060</th>\n",
              "      <td>0.675503</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.258832</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092345</td>\n",
              "      <td>0.174853</td>\n",
              "      <td>0.091216</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090220</td>\n",
              "      <td>0.002104</td>\n",
              "      <td>0.004293</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114061</th>\n",
              "      <td>0.681982</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.258795</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092345</td>\n",
              "      <td>0.175544</td>\n",
              "      <td>0.091139</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090220</td>\n",
              "      <td>0.002103</td>\n",
              "      <td>0.004318</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114062</th>\n",
              "      <td>0.680337</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.259524</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092284</td>\n",
              "      <td>0.173561</td>\n",
              "      <td>0.091090</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090213</td>\n",
              "      <td>0.002103</td>\n",
              "      <td>0.004410</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114063</th>\n",
              "      <td>0.695403</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.259524</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092284</td>\n",
              "      <td>0.173561</td>\n",
              "      <td>0.091090</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090213</td>\n",
              "      <td>0.002102</td>\n",
              "      <td>0.004454</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114064</th>\n",
              "      <td>0.699646</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.259524</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092284</td>\n",
              "      <td>0.175470</td>\n",
              "      <td>0.091090</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090213</td>\n",
              "      <td>0.002102</td>\n",
              "      <td>0.004425</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114065</th>\n",
              "      <td>0.714108</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.259524</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092284</td>\n",
              "      <td>0.184883</td>\n",
              "      <td>0.091089</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090213</td>\n",
              "      <td>0.002103</td>\n",
              "      <td>0.004484</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114066</th>\n",
              "      <td>0.711624</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.259524</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092284</td>\n",
              "      <td>0.184883</td>\n",
              "      <td>0.091089</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090218</td>\n",
              "      <td>0.002102</td>\n",
              "      <td>0.004496</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114067</th>\n",
              "      <td>0.719332</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.259524</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092284</td>\n",
              "      <td>0.184883</td>\n",
              "      <td>0.091089</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090213</td>\n",
              "      <td>0.002103</td>\n",
              "      <td>0.004493</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114068</th>\n",
              "      <td>0.718780</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.259524</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092314</td>\n",
              "      <td>0.189594</td>\n",
              "      <td>0.091090</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090213</td>\n",
              "      <td>0.002102</td>\n",
              "      <td>0.004564</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114069</th>\n",
              "      <td>0.707893</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.259524</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092314</td>\n",
              "      <td>0.189594</td>\n",
              "      <td>0.091090</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090213</td>\n",
              "      <td>0.002102</td>\n",
              "      <td>0.004575</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114070</th>\n",
              "      <td>0.710446</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.259524</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092314</td>\n",
              "      <td>0.189349</td>\n",
              "      <td>0.091090</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090214</td>\n",
              "      <td>0.002103</td>\n",
              "      <td>0.004461</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114071</th>\n",
              "      <td>0.698587</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.259524</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092357</td>\n",
              "      <td>0.189349</td>\n",
              "      <td>0.091080</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090214</td>\n",
              "      <td>0.002103</td>\n",
              "      <td>0.004424</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114072</th>\n",
              "      <td>0.696699</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.259524</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092388</td>\n",
              "      <td>0.189349</td>\n",
              "      <td>0.091080</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090214</td>\n",
              "      <td>0.002103</td>\n",
              "      <td>0.004640</td>\n",
              "      <td>0.066109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114073</th>\n",
              "      <td>0.684794</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.259524</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092388</td>\n",
              "      <td>0.189349</td>\n",
              "      <td>0.091080</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090215</td>\n",
              "      <td>0.002103</td>\n",
              "      <td>0.004595</td>\n",
              "      <td>0.066110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114074</th>\n",
              "      <td>0.679763</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.260650</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092348</td>\n",
              "      <td>0.191254</td>\n",
              "      <td>0.091080</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090214</td>\n",
              "      <td>0.002103</td>\n",
              "      <td>0.004946</td>\n",
              "      <td>0.066118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114075</th>\n",
              "      <td>0.664852</td>\n",
              "      <td>0.455579</td>\n",
              "      <td>0.285245</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092297</td>\n",
              "      <td>0.188561</td>\n",
              "      <td>0.091080</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090215</td>\n",
              "      <td>0.002102</td>\n",
              "      <td>0.004724</td>\n",
              "      <td>0.066119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114076</th>\n",
              "      <td>0.558316</td>\n",
              "      <td>0.461605</td>\n",
              "      <td>0.320690</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092100</td>\n",
              "      <td>0.178483</td>\n",
              "      <td>0.091080</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090226</td>\n",
              "      <td>0.002101</td>\n",
              "      <td>0.004961</td>\n",
              "      <td>0.066119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114077</th>\n",
              "      <td>0.398056</td>\n",
              "      <td>0.467319</td>\n",
              "      <td>0.366394</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092067</td>\n",
              "      <td>0.158155</td>\n",
              "      <td>0.091080</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090228</td>\n",
              "      <td>0.002094</td>\n",
              "      <td>0.004967</td>\n",
              "      <td>0.066119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114078</th>\n",
              "      <td>0.282438</td>\n",
              "      <td>0.466782</td>\n",
              "      <td>0.397136</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>0.092044</td>\n",
              "      <td>0.153811</td>\n",
              "      <td>0.091080</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.090228</td>\n",
              "      <td>0.002094</td>\n",
              "      <td>0.004955</td>\n",
              "      <td>0.066119</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              0         1         2   ...        9         10        11\n",
              "114029  0.374518  0.465062  0.405746  ...  0.002093  0.005089  0.066109\n",
              "114030  0.388508  0.465062  0.392066  ...  0.002094  0.005466  0.066117\n",
              "114031  0.405816  0.465062  0.351830  ...  0.002093  0.005339  0.066118\n",
              "114032  0.537770  0.459445  0.329910  ...  0.002101  0.005694  0.066118\n",
              "114033  0.687965  0.453513  0.262351  ...  0.002101  0.005809  0.066118\n",
              "114034  0.703408  0.453513  0.260091  ...  0.002103  0.005768  0.066118\n",
              "114035  0.714621  0.453513  0.258536  ...  0.002105  0.005918  0.066118\n",
              "114036  0.721492  0.453513  0.257109  ...  0.002106  0.005733  0.066118\n",
              "114037  0.717253  0.453513  0.258516  ...  0.002106  0.005678  0.066118\n",
              "114038  0.708699  0.453513  0.260067  ...  0.002104  0.005549  0.066118\n",
              "114039  0.701997  0.453513  0.261541  ...  0.002102  0.005244  0.066117\n",
              "114040  0.660874  0.453513  0.288999  ...  0.002102  0.004910  0.066109\n",
              "114041  0.596478  0.454251  0.309315  ...  0.002102  0.004942  0.066109\n",
              "114042  0.589309  0.454251  0.309898  ...  0.002102  0.004957  0.066109\n",
              "114043  0.654395  0.453513  0.288317  ...  0.002102  0.005046  0.066117\n",
              "114044  0.697266  0.455579  0.260309  ...  0.002102  0.005051  0.066118\n",
              "114045  0.708183  0.455579  0.259032  ...  0.002103  0.005115  0.066118\n",
              "114046  0.716434  0.455579  0.258161  ...  0.002105  0.004852  0.066118\n",
              "114047  0.720567  0.455579  0.257318  ...  0.002105  0.004902  0.066118\n",
              "114048  0.719337  0.455579  0.258288  ...  0.002104  0.004856  0.066117\n",
              "114049  0.716374  0.455579  0.259108  ...  0.002103  0.004694  0.066111\n",
              "114050  0.702788  0.455579  0.261113  ...  0.002102  0.004501  0.066109\n",
              "114051  0.664042  0.455579  0.296730  ...  0.002102  0.004459  0.066109\n",
              "114052  0.613152  0.456333  0.312023  ...  0.002102  0.004429  0.066109\n",
              "114053  0.559720  0.461605  0.322084  ...  0.002102  0.004425  0.066109\n",
              "114054  0.524940  0.461605  0.326428  ...  0.002101  0.004436  0.066109\n",
              "114055  0.551177  0.461605  0.320822  ...  0.002102  0.004219  0.066109\n",
              "114056  0.616322  0.455579  0.305963  ...  0.002102  0.003897  0.066109\n",
              "114057  0.685683  0.455579  0.260517  ...  0.002102  0.003845  0.066109\n",
              "114058  0.685809  0.455579  0.258795  ...  0.002104  0.003818  0.066109\n",
              "114059  0.685725  0.455579  0.258384  ...  0.002104  0.004304  0.066109\n",
              "114060  0.675503  0.455579  0.258832  ...  0.002104  0.004293  0.066109\n",
              "114061  0.681982  0.455579  0.258795  ...  0.002103  0.004318  0.066109\n",
              "114062  0.680337  0.455579  0.259524  ...  0.002103  0.004410  0.066109\n",
              "114063  0.695403  0.455579  0.259524  ...  0.002102  0.004454  0.066109\n",
              "114064  0.699646  0.455579  0.259524  ...  0.002102  0.004425  0.066109\n",
              "114065  0.714108  0.455579  0.259524  ...  0.002103  0.004484  0.066109\n",
              "114066  0.711624  0.455579  0.259524  ...  0.002102  0.004496  0.066109\n",
              "114067  0.719332  0.455579  0.259524  ...  0.002103  0.004493  0.066109\n",
              "114068  0.718780  0.455579  0.259524  ...  0.002102  0.004564  0.066109\n",
              "114069  0.707893  0.455579  0.259524  ...  0.002102  0.004575  0.066109\n",
              "114070  0.710446  0.455579  0.259524  ...  0.002103  0.004461  0.066109\n",
              "114071  0.698587  0.455579  0.259524  ...  0.002103  0.004424  0.066109\n",
              "114072  0.696699  0.455579  0.259524  ...  0.002103  0.004640  0.066109\n",
              "114073  0.684794  0.455579  0.259524  ...  0.002103  0.004595  0.066110\n",
              "114074  0.679763  0.455579  0.260650  ...  0.002103  0.004946  0.066118\n",
              "114075  0.664852  0.455579  0.285245  ...  0.002102  0.004724  0.066119\n",
              "114076  0.558316  0.461605  0.320690  ...  0.002101  0.004961  0.066119\n",
              "114077  0.398056  0.467319  0.366394  ...  0.002094  0.004967  0.066119\n",
              "114078  0.282438  0.466782  0.397136  ...  0.002094  0.004955  0.066119\n",
              "\n",
              "[50 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 434
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5dSrWT2pI-1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "eb7aa239-70d8-4006-c0de-656c8c1583bf"
      },
      "source": [
        "valid1_lithology.tail(50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "114029    2\n",
              "114030    2\n",
              "114031    2\n",
              "114032    2\n",
              "114033    2\n",
              "114034    2\n",
              "114035    2\n",
              "114036    2\n",
              "114037    2\n",
              "114038    2\n",
              "114039    2\n",
              "114040    2\n",
              "114041    2\n",
              "114042    2\n",
              "114043    2\n",
              "114044    2\n",
              "114045    2\n",
              "114046    2\n",
              "114047    2\n",
              "114048    2\n",
              "114049    2\n",
              "114050    2\n",
              "114051    2\n",
              "114052    2\n",
              "114053    2\n",
              "114054    2\n",
              "114055    2\n",
              "114056    2\n",
              "114057    2\n",
              "114058    2\n",
              "114059    2\n",
              "114060    2\n",
              "114061    2\n",
              "114062    2\n",
              "114063    2\n",
              "114064    2\n",
              "114065    2\n",
              "114066    2\n",
              "114067    2\n",
              "114068    2\n",
              "114069    2\n",
              "114070    2\n",
              "114071    2\n",
              "114072    2\n",
              "114073    2\n",
              "114074    2\n",
              "114075    2\n",
              "114076    2\n",
              "114077    2\n",
              "114078    2\n",
              "Name: FORCE_2020_LITHOFACIES_LITHOLOGY, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 250
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRt9aEibLud2"
      },
      "source": [
        " stack = np.column_stack((pred1, pred2, pred3, pred4, pred5, pred6, pred7, pred8))\n",
        "stack_p = np.column_stack((val1, val2, val3, val4, val5, val6, val7, val8))\n",
        "stack_open = np.column_stack((open_test1, open_test2, open_test3, open_test4, open_test5, open_test6, open_test7, open_test8))\n",
        "stack_open1 = np.column_stack((open_test11, open_test22, open_test33, open_test44, open_test55, open_test66, open_test77, open_test88))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSMvSIHzMpvz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2552d632-1415-4568-dc60-7e6770a78c8b"
      },
      "source": [
        "np.unique(open_test7, return_counts=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0]), array([91467]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foDqil9ZMw4x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "fdad021d-53e3-48a4-e6c2-2934ef62867c"
      },
      "source": [
        "meta_model = LogisticRegression()\n",
        "#meta_model = RandomForestClassifier(n_estimators=100, random_state=890, verbose=2, class_weight='balanced')\n",
        "#meta_model = ExtraTreesClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "meta_model.fit(stack, valid_target)\n",
        "final_val_pred = meta_model.predict(stack_p)\n",
        "final_open_pred = meta_model.predict(stack_p)\n",
        "final_open_pred1 = meta_model.predict(stack_open)\n",
        "final_open_pred11 = meta_model.predict(stack_open1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Df0jraJzXFK-"
      },
      "source": [
        "final_open_pred = pd.DataFrame(final_open_pred)\n",
        "final_open_pred1 = pd.DataFrame(final_open_pred1)\n",
        "final_open_pred11 = pd.DataFrame(final_open_pred11)\n",
        "    \n",
        "final_open_pred = np.array(pd.DataFrame(final_open_pred).idxmax(axis=1))\n",
        "final_open_pred1 = np.array(pd.DataFrame(final_open_pred1).idxmax(axis=1))\n",
        "final_open_pred11 = np.array(pd.DataFrame(final_open_pred11).idxmax(axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t9ICNzBnXJJ"
      },
      "source": [
        "LOGISTIC REGRESSION META STACKER RESULT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9z39ahUPM3_L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "a492434e-a212-458e-847b-bdb4a5f6bb49"
      },
      "source": [
        "print(show_evaluation(final_open_pred, valid1_lithology))\n",
        "print(show_evaluation(final_open_pred1, valid2_lithology))\n",
        "print(show_evaluation(final_open_pred11, valid3_lithology))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default score: -0.8393591283233549\n",
            "Accuracy is: 0.7080005960781564\n",
            "F1 is: 0.7851857424717705\n",
            "None\n",
            "Default score: -0.6946057047897056\n",
            "Accuracy is: 0.7555949140127041\n",
            "F1 is: 0.8240832920449708\n",
            "None\n",
            "Default score: -0.7749445379623052\n",
            "Accuracy is: 0.7291798429548617\n",
            "F1 is: 0.8019078017608368\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beHZTqGCY-ii",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "05eaefac-3ef1-4092-bfdb-600f5c583570"
      },
      "source": [
        "print(show_evaluation(final_open_pred, valid1_lithology))\n",
        "print(show_evaluation(final_open_pred1, valid2_lithology))\n",
        "print(show_evaluation(final_open_pred11, valid3_lithology))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default score: -0.8129443192875113\n",
            "Accuracy is: 0.7139789093522909\n",
            "F1 is: 0.7741328642741403\n",
            "None\n",
            "Default score: -0.6669850875179026\n",
            "Accuracy is: 0.761848535537407\n",
            "F1 is: 0.8246254310271689\n",
            "None\n",
            "Default score: -0.7479931499518356\n",
            "Accuracy is: 0.7352806671012815\n",
            "F1 is: 0.7948853606804036\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owLlvBnZcCHI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "91a4e293-a0ab-4db8-b826-f6e79ebcfef5"
      },
      "source": [
        "stack_p.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(114079, 48)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC_N4smEipBZ"
      },
      "source": [
        "class Model():\n",
        "    \n",
        "    def __init__(self, train, validation1, validation2, validation3, train_label, valid_label1, valid_label2, valid_label3):\n",
        "        \n",
        "        \n",
        "        self.train = train\n",
        "        #self.test = test\n",
        "        self.validation1 = validation1\n",
        "        self.validation2 = validation2\n",
        "        self.validation3 = validation3\n",
        "        self.train_label = train_label\n",
        "        self.valid_label1 = valid_label1\n",
        "        self.valid_label2 = valid_label2\n",
        "        self.valid_label3 = valid_label3\n",
        "        \n",
        "    def __call__(self, plot = True):\n",
        "        return self.fit(plot)\n",
        "    \n",
        "    def fit(self, plot):\n",
        " \n",
        "      #self.x_train, self.x_test, self.y_train, self.y_test = ms.train_test_split(self.train, \n",
        "                                                                                  # pd.DataFrame(np.array(self.train_label)), \n",
        "                                                                                   #test_size=0.2,\n",
        "                                                                                   #random_state=212)\n",
        "      #self.x_train = self.train.iloc[:700000]\n",
        "      #self.x_test = self.train.iloc[700000:]\n",
        "      #self.y_train = pd.DataFrame(self.train_label).iloc[:700000]\n",
        "      #self.y_test = pd.DataFrame(self.train_label).iloc[700000:]\n",
        "      \n",
        "      def show_evaluation(pred, true):\n",
        "        \n",
        "        print(f'Default score: {score(true.values, pred)}')\n",
        "        print(f'Accuracy is: {accuracy_score(true, pred)}')\n",
        "        print(f'F1 is: {f1_score(pred, true.values, average=\"weighted\")}')\n",
        "      split = 5\n",
        "      kf = StratifiedKFold(n_splits=split, shuffle=True)\n",
        "  \n",
        "      #pred = np.zeros((len(self.test), 12))\n",
        "      val1 = np.zeros((len(self.validation1), 12))\n",
        "      val2 = np.zeros((len(self.validation2), 12))\n",
        "      val3 = np.zeros((len(self.validation3), 12))\n",
        " \n",
        "      #model = CatBoostClassifier(n_estimators=5000, random_state=2020, learning_rate=0.033,\n",
        "                                 #use_best_model=True, max_depth=10, reg_lambda=1500,\n",
        "                                 #eval_metric='MultiClass', task_type='GPU', verbose=100)\n",
        " \n",
        "      model = XGBClassifier(n_estimators=100, max_depth=10, reg_lambda=1200, booster_type='gbtree',\n",
        "                            objective='multi:softprob', learning_rate=0.1, random_state=2020,\n",
        "                            subsample=0.7, col_sample_bytree=0.9, tree_method='gpu_hist',\n",
        "                            eval_metric='mlogloss', verbose=2020)\n",
        "      \n",
        "      #model = LGBMClassifier(n_estimators=50000, max_depth=10, reg_lambda=200,\n",
        "                            #objective='multiclass', learning_rate=0.033,\n",
        "                            #eval_metric='multi_logloss')\n",
        "      \n",
        "      #model = RandomForestClassifier(n_estimators=100, class_weight='balanced', verbose=2)\n",
        "      i = 1\n",
        "      for (train_index, test_index) in kf.split(pd.DataFrame(traindata), pd.DataFrame(lithology)):\n",
        "        X_train,X_test = pd.DataFrame(traindata).iloc[train_index], pd.DataFrame(traindata).iloc[test_index]\n",
        "        Y_train,Y_test = pd.DataFrame(lithology).iloc[train_index],pd.DataFrame(lithology).iloc[test_index]\n",
        "    \n",
        "        \n",
        "        model.fit(X_train, Y_train, early_stopping_rounds=100, eval_set=[(X_test, Y_test)], verbose=100)\n",
        "        #model.fit(X_train, Y_train)\n",
        "        prediction1 = model.predict(pd.DataFrame(self.validation1))\n",
        "        prediction = model.predict(pd.DataFrame(self.validation2))\n",
        "        print(show_evaluation(prediction1, self.valid_label1))\n",
        "        print(show_evaluation(prediction, self.valid_label2))\n",
        " \n",
        "        print(f'-----------------------FOLD {i}---------------------')\n",
        "        i+=1\n",
        " \n",
        "        #pred += model.predict_proba(self.test)\n",
        "        val1 += model.predict_proba(pd.DataFrame(self.validation1))\n",
        "        val2 += model.predict_proba(pd.DataFrame(self.validation2))\n",
        "        val3 += model.predict_proba(pd.DataFrame(self.validation3))\n",
        "      \n",
        "      #pred = pd.DataFrame(pred/split)\n",
        "      val1 = pd.DataFrame(val1/split)\n",
        "      val2 = pd.DataFrame(val2/split)\n",
        "      val3 = pd.DataFrame(val3/split)\n",
        "    \n",
        "      #pred = np.array(pd.DataFrame(pred).idxmax(axis=1))\n",
        "      val1 = np.array(pd.DataFrame(val1).idxmax(axis=1))\n",
        "      val2 = np.array(pd.DataFrame(val2).idxmax(axis=1))\n",
        "      val3 = np.array(pd.DataFrame(val3).idxmax(axis=1))\n",
        " \n",
        "      print('---------------CROSS VALIDATION COMPLETE')\n",
        "      print('----------------TEST EVALUATION------------------')\n",
        " \n",
        "      print('----------Valid 1-------------')\n",
        "      print(show_evaluation(val1, self.valid_label1))\n",
        "      print('----------Valid 2-------------')\n",
        "      print(show_evaluation(val2, self.valid_label2))\n",
        "      print('----------Valid 3-------------')\n",
        "      print(show_evaluation(val3, self.valid_label3))\n",
        "                  \n",
        "      if plot: self.plot_feat_imp(model)\n",
        "      return val1, val2, val3, model\n",
        "              \n",
        "              \n",
        "    def plot_feat_imp(self, model):\n",
        "        feat_imp = pd.Series(model.feature_importances_).sort_values(ascending=False)\n",
        "        plt.figure(figsize=(12,8))\n",
        "        feat_imp.plot(kind='bar', title='Feature Importances')\n",
        "        plt.ylabel('Feature Importance Score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5jCsqvDbypf",
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9ca4603f-9e2a-418f-b224-538ee55cd0a0"
      },
      "source": [
        "func1_= Model(traindata, validation1, validation2, validation3, lithology, valid1_lithology, valid2_lithology, valid3_lithology)\n",
        "val1, open_test1, open_test11, model1 = func1_()   #no reglambda, 0.01 lr, 10max-depth"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0]\tvalidation_0-mlogloss:2.37987\n",
            "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
            "[100]\tvalidation_0-mlogloss:0.528698\n",
            "[200]\tvalidation_0-mlogloss:0.36638\n",
            "[299]\tvalidation_0-mlogloss:0.311491\n",
            "Default score: -0.6154145372943312\n",
            "Accuracy is: 0.7688268655931416\n",
            "F1 is: 0.8040155873188836\n",
            "None\n",
            "Default score: -0.5592987088239474\n",
            "Accuracy is: 0.7917281642559612\n",
            "F1 is: 0.8185497760510676\n",
            "None\n",
            "-----------------------FOLD 1---------------------\n",
            "[0]\tvalidation_0-mlogloss:2.37999\n",
            "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
            "[100]\tvalidation_0-mlogloss:0.525108\n",
            "[200]\tvalidation_0-mlogloss:0.362007\n",
            "[299]\tvalidation_0-mlogloss:0.307947\n",
            "Default score: -0.6165979277518211\n",
            "Accuracy is: 0.7687041436197722\n",
            "F1 is: 0.8036775928572035\n",
            "None\n",
            "Default score: -0.5567199099128648\n",
            "Accuracy is: 0.7928105218275444\n",
            "F1 is: 0.8191677659718717\n",
            "None\n",
            "-----------------------FOLD 2---------------------\n",
            "[0]\tvalidation_0-mlogloss:2.37978\n",
            "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
            "[100]\tvalidation_0-mlogloss:0.525896\n",
            "[200]\tvalidation_0-mlogloss:0.362994\n",
            "[299]\tvalidation_0-mlogloss:0.307957\n",
            "Default score: -0.620317937569579\n",
            "Accuracy is: 0.7668282505982696\n",
            "F1 is: 0.8020791974838445\n",
            "None\n",
            "Default score: -0.5566529458711885\n",
            "Accuracy is: 0.7927667902084905\n",
            "F1 is: 0.8194996212924677\n",
            "None\n",
            "-----------------------FOLD 3---------------------\n",
            "[0]\tvalidation_0-mlogloss:2.37967\n",
            "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
            "[100]\tvalidation_0-mlogloss:0.527983\n",
            "[200]\tvalidation_0-mlogloss:0.364027\n",
            "[299]\tvalidation_0-mlogloss:0.309038\n",
            "Default score: -0.622789908747447\n",
            "Accuracy is: 0.7660480894818503\n",
            "F1 is: 0.8014258235479497\n",
            "None\n",
            "Default score: -0.5515964774180853\n",
            "Accuracy is: 0.7951501634469262\n",
            "F1 is: 0.82148450563001\n",
            "None\n",
            "-----------------------FOLD 4---------------------\n",
            "[0]\tvalidation_0-mlogloss:2.37961\n",
            "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
            "[100]\tvalidation_0-mlogloss:0.524414\n",
            "[200]\tvalidation_0-mlogloss:0.36068\n",
            "[299]\tvalidation_0-mlogloss:0.305766\n",
            "Default score: -0.6122544464800709\n",
            "Accuracy is: 0.769633324275283\n",
            "F1 is: 0.8058013215722035\n",
            "None\n",
            "Default score: -0.5621781626160255\n",
            "Accuracy is: 0.7905255447319799\n",
            "F1 is: 0.8161528262284781\n",
            "None\n",
            "-----------------------FOLD 5---------------------\n",
            "[0]\tvalidation_0-mlogloss:2.37972\n",
            "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
            "[100]\tvalidation_0-mlogloss:0.526115\n",
            "[200]\tvalidation_0-mlogloss:0.36235\n",
            "[299]\tvalidation_0-mlogloss:0.307018\n",
            "Default score: -0.619897176518027\n",
            "Accuracy is: 0.7672402457945809\n",
            "F1 is: 0.8030714689583137\n",
            "None\n",
            "Default score: -0.55755627712727\n",
            "Accuracy is: 0.7921436146369729\n",
            "F1 is: 0.818838228818283\n",
            "None\n",
            "-----------------------FOLD 6---------------------\n",
            "[0]\tvalidation_0-mlogloss:2.38012\n",
            "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
            "[100]\tvalidation_0-mlogloss:0.522759\n",
            "[200]\tvalidation_0-mlogloss:0.358574\n",
            "[299]\tvalidation_0-mlogloss:0.303558\n",
            "Default score: -0.6094307453606711\n",
            "Accuracy is: 0.7707027586146442\n",
            "F1 is: 0.8057127697595593\n",
            "None\n",
            "Default score: -0.556601014573562\n",
            "Accuracy is: 0.7927230585894366\n",
            "F1 is: 0.8190388387420483\n",
            "None\n",
            "-----------------------FOLD 7---------------------\n",
            "[0]\tvalidation_0-mlogloss:2.37984\n",
            "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
            "[100]\tvalidation_0-mlogloss:0.526426\n",
            "[200]\tvalidation_0-mlogloss:0.364458\n",
            "[299]\tvalidation_0-mlogloss:0.310973\n",
            "Default score: -0.6198401984589627\n",
            "Accuracy is: 0.7674418604651163\n",
            "F1 is: 0.8042966469810168\n",
            "None\n",
            "Default score: -0.5549091475614156\n",
            "Accuracy is: 0.7928214547323078\n",
            "F1 is: 0.8192849694482026\n",
            "None\n",
            "-----------------------FOLD 8---------------------\n",
            "[0]\tvalidation_0-mlogloss:2.37974\n",
            "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
            "[100]\tvalidation_0-mlogloss:0.527541\n",
            "[200]\tvalidation_0-mlogloss:0.363598\n",
            "[299]\tvalidation_0-mlogloss:0.308101\n",
            "Default score: -0.61756984195163\n",
            "Accuracy is: 0.768239553292017\n",
            "F1 is: 0.8040856408922035\n",
            "None\n",
            "Default score: -0.5548968480435567\n",
            "Accuracy is: 0.7942645981610854\n",
            "F1 is: 0.8195749915048841\n",
            "None\n",
            "-----------------------FOLD 9---------------------\n",
            "[0]\tvalidation_0-mlogloss:2.37946\n",
            "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
            "[100]\tvalidation_0-mlogloss:0.525815\n",
            "[200]\tvalidation_0-mlogloss:0.361771\n",
            "[299]\tvalidation_0-mlogloss:0.307111\n",
            "Default score: -0.6166110765346821\n",
            "Accuracy is: 0.768213255726295\n",
            "F1 is: 0.8047123075817252\n",
            "None\n",
            "Default score: -0.5591442815441635\n",
            "Accuracy is: 0.791793761684542\n",
            "F1 is: 0.8176891224412887\n",
            "None\n",
            "-----------------------FOLD 10---------------------\n",
            "---------------CROSS VALIDATION COMPLETE\n",
            "----------------TEST EVALUATION------------------\n",
            "----------Valid 1-------------\n",
            "Default score: -0.6149335109879995\n",
            "Accuracy is: 0.7689671192769922\n",
            "F1 is: 0.8047471181384308\n",
            "None\n",
            "----------Valid 2-------------\n",
            "Default score: -0.5560010714246668\n",
            "Accuracy is: 0.793083844446631\n",
            "F1 is: 0.8193609702641306\n",
            "None\n",
            "----------Valid 3-------------\n",
            "Default score: -0.5887088534926489\n",
            "Accuracy is: 0.7796989481673202\n",
            "F1 is: 0.8107391304306448\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtgAAAHlCAYAAADP34vrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3defzlZHnw/88FCIKswrixDQqiuGARQatW1KooKi5QccWt1CoiVWvxURGpteBTtX0etb9i0SrWIlKXaUERxeVxLciwOCI6juyIAwzbsA5cvz+SL2bOnNwn851kZs7M5/16ndf3nCR3ciW5k1zn/ib3icxEkiRJUj82WNMBSJIkSesSE2xJkiSpRybYkiRJUo9MsCVJkqQemWBLkiRJPTLBliRJknpkgi1JkiT1yARbkmoRcUlE3BYRtzReD+lhnn/aV4wdlndMRHx+dS2vJCJeGxE/WNNxSNLqZoItSct7QWZu3nhdtSaDiYiN1uTyZ2ta45akPphgS9IEEbFVRJwYEVdHxJUR8cGI2LAe97CIOCsirouIayPi3yNi63rcScBOwH/VreHvioj9IuKKkfnf28pdt0CfGhGfj4ibgNeWlt8h9oyIN0fEryPi5oj42zrmH0XETRFxSkRsXE+7X0RcERH/q16XSyLilSPb4XMRsTgiLo2I90bEBvW410bEDyPiYxFxHfBF4P8DnlSv+w31dAdExPx62ZdHxDGN+c+t4z00Ii6rY3hPY/yGdWy/qdflZxGxYz3uERFxZkRcHxEXR8SfNco9LyJ+UZe5MiLe2XnnS9IsmGBL0mT/BiwDdgX+CHg28MZ6XAB/DzwEeCSwI3AMQGa+GriMP7SKf7jj8g4ETgW2Bv59wvK7eA7weOCJwLuAE4BX1bE+Gnh5Y9oHAdsB2wOHAidExO71uP8LbAU8FHga8BrgdY2y+wKLgAfW838T8ON63beup1lal9saOAD4y4h40Ui8TwF2B54JHB0Rj6yHv72O9XnAlsDrgVsj4n7AmcAXgAcAhwCfjIg96nInAn+RmVvU63tWp60mSbNkgi1Jy/tqRNxQv74aEQ+kSuiOzMylmfl74GNUSRyZuTAzz8zMOzJzMfBRquRzVfw4M7+amfdQJZKty+/ow5l5U2YuAH4OfDMzF2XmjcDXqZL2pvfV6/M94DTgz+oW80OAd2fmzZl5CfAR4NWNcldl5v/NzGWZedu4QDLzu5l5YWbek5kXAP/BitvrA5l5W2aeD5wP7FkPfyPw3sy8OCvnZ+Z1wPOBSzLzM/Wy5wP/CRxcl7sL2CMitszMJZl57kpsO0laad4jJ0nLe1FmfmvmQ0TsA9wHuDoiZgZvAFxej38g8E/AU4Et6nFLVjGGyxvvdy4tv6NrGu9vG/P5QY3PSzJzaePzpVSt89vVcVw6Mm77lrjHioh9geOoWpI3BjYBvjQy2e8a728FNq/f7wj8Zsxsdwb2nbkNpbYRcFL9/qXAe4HjIuIC4KjM/PGkWCVptmzBlqSyy4E7gO0yc+v6tWVmPqoe/yEggcdk5pZUt0ZEo3yOzG8psNnMh7pleM7INM0yk5bft23qWy5m7ARcBVxL1RK888i4K1viHvcZqts45gE7ZuZWVPdpx5jpxrkceFjL8O81ts/W9W0pfwmQmWdn5oFUt498FTil4/IkaVZMsCWpIDOvBr4JfCQitoyIDeqHBGdua9gCuAW4MSK2B/56ZBbXUN2zPONXwH3rh/3uQ9WyuskqLH8IH4iIjSPiqVS3X3wpM++mSkz/LiK2iIidqe6JLnUJeA2ww8xDlLUtgOsz8/b6vwOvWIm4/hX424jYLSqPjYhtgf8GHh4Rr46I+9SvJ0TEI+v1eGVEbJWZdwE3AfesxDIlaaWZYEvSZK+hup3hF1S3f5wKPLge9wFgL+BGqvuVvzxS9u+B99b3dL+zvu/5zVTJ4pVULdpXUFZaft9+Vy/jKqoHLN+Umb+sx72VKt5FwA+oWqM/XZjXWcAC4HcRcW097M3AsRFxM3A0K9ea/NF6+m9SJconAptm5s1UD34eUsf9O+B4/vDF5dXAJXWvLG8CXokkDSgyx/0HT5K0vomI/YDPZ+YOazoWSZpmtmBLkiRJPTLBliRJknrkLSKSJElSj2zBliRJknpkgi1JkiT1aJ35Jcftttsu586du6bDkCRJ0jruZz/72bWZOfojYfdaZxLsuXPncs4556zpMCRJkrSOi4hLS+O9RUSSJEnqkQm2JEmS1CMTbEmSJKlHJtiSJElSj0ywJUmSpB6ZYEuSJEk9MsGWJEmSemSCLUmSJPXIBFuSJEnqkQm2JEmS1CMTbEmSJKlHJtiSJElSj0ywJUmSpB4NmmBHxP4RcXFELIyIo8aM/5OIODcilkXEQSPjDo2IX9evQ4eMU5IkSerLYAl2RGwIfAJ4LrAH8PKI2GNkssuA1wJfGCl7f+D9wL7APsD7I2KboWKVJEmS+jJkC/Y+wMLMXJSZdwInAwc2J8jMSzLzAuCekbLPAc7MzOszcwlwJrD/gLFKkiRJvRgywd4euLzx+Yp62NBlJUmSpDVmqh9yjIjDIuKciDhn8eLFazocSZIkadAE+0pgx8bnHephvZXNzBMyc+/M3HvOnDmzDlSSJEnqy0YDzvtsYLeI2IUqOT4EeEXHsmcAH2o82Phs4N1dCs496rTlPl9y3AEdFylJkiStusFasDNzGXA4VbJ8EXBKZi6IiGMj4oUAEfGEiLgCOBj4l4hYUJe9HvhbqiT9bODYepgkSZK0VhuyBZvMPB04fWTY0Y33Z1Pd/jGu7KeBTw8ZnyRJktS3qX7IUZIkSVrbmGBLkiRJPTLBliRJknpkgi1JkiT1yARbkiRJ6pEJtiRJktQjE2xJkiSpRybYkiRJUo9MsCVJkqQemWBLkiRJPTLBliRJknpkgi1JkiT1yARbkiRJ6pEJtiRJktQjE2xJkiSpRybYkiRJUo9MsCVJkqQemWBLkiRJPTLBliRJknpkgi1JkiT1yARbkiRJ6pEJtiRJktQjE2xJkiSpRybYkiRJUo9MsCVJkqQemWBLkiRJPTLBliRJknpkgi1JkiT1yARbkiRJ6pEJtiRJktQjE2xJkiSpRybYkiRJUo9MsCVJkqQemWBLkiRJPTLBliRJknpkgi1JkiT1yARbkiRJ6pEJtiRJktQjE2xJkiSpRybYkiRJUo9MsCVJkqQemWBLkiRJPTLBliRJknpkgi1JkiT1yARbkiRJ6pEJtiRJktQjE2xJkiSpRybYkiRJUo9MsCVJkqQemWBLkiRJPTLBliRJknpkgi1JkiT1yARbkiRJ6pEJtiRJktQjE2xJkiSpRybYkiRJUo9MsCVJkqQemWBLkiRJPTLBliRJknpkgi1JkiT1aKM1HcDqNPeo05b7fMlxB6yhSCRJkrSusgVbkiRJ6pEJtiRJktQjE2xJkiSpRybYkiRJUo9MsCVJkqQemWBLkiRJPTLBliRJknpkgi1JkiT1yARbkiRJ6tF69UuOkzR/6dFfeZQkSdJs2IItSZIk9cgEW5IkSeqRCbYkSZLUo0ET7IjYPyIujoiFEXHUmPGbRMQX6/E/jYi59fD7RMRnI+LCiLgoIt49ZJySJElSXwZLsCNiQ+ATwHOBPYCXR8QeI5O9AViSmbsCHwOOr4cfDGySmY8BHg/8xUzyLUmSJK3NhmzB3gdYmJmLMvNO4GTgwJFpDgQ+W78/FXhmRASQwP0iYiNgU+BO4KYBY5UkSZJ6MWSCvT1weePzFfWwsdNk5jLgRmBbqmR7KXA1cBnwD5l5/YCxSpIkSb1YWx9y3Ae4G3gIsAvwjoh46OhEEXFYRJwTEecsXrx4dccoSZIkrWDIBPtKYMfG5x3qYWOnqW8H2Qq4DngF8I3MvCszfw/8ENh7dAGZeUJm7p2Ze8+ZM2eAVZAkSZJWzpAJ9tnAbhGxS0RsDBwCzBuZZh5waP3+IOCszEyq20KeARAR9wOeCPxywFglSZKkXgyWYNf3VB8OnAFcBJySmQsi4tiIeGE92YnAthGxEHg7MNOV3yeAzSNiAVWi/pnMvGCoWCVJkqS+bDTkzDPzdOD0kWFHN97fTtUl32i5W8YNlyRJktZ2a+tDjpIkSdJUMsGWJEmSemSCLUmSJPXIBFuSJEnqkQm2JEmS1CMTbEmSJKlHJtiSJElSj0ywJUmSpB6ZYEuSJEk9MsGWJEmSemSCLUmSJPXIBFuSJEnqkQm2JEmS1CMTbEmSJKlHJtiSJElSj0ywJUmSpB6ZYEuSJEk9MsGWJEmSemSCLUmSJPXIBFuSJEnqkQm2JEmS1CMTbEmSJKlHJtiSJElSj0ywJUmSpB6ZYEuSJEk9MsGWJEmSemSCLUmSJPXIBFuSJEnqkQm2JEmS1CMTbEmSJKlHJtiSJElSj0ywJUmSpB6ZYEuSJEk9MsGWJEmSemSCLUmSJPXIBFuSJEnqkQm2JEmS1CMTbEmSJKlHJtiSJElSj0ywJUmSpB6ZYEuSJEk9MsGWJEmSemSCLUmSJPXIBFuSJEnqkQm2JEmS1CMTbEmSJKlHJtiSJElSjzon2BGx2ZCBSJIkSeuCiQl2RPxxRPwC+GX9ec+I+OTgkUmSJElTqEsL9seA5wDXAWTm+cCfDBmUJEmSNK063SKSmZePDLp7gFgkSZKkqbdRh2kuj4g/BjIi7gO8Dbho2LAkSZKk6dSlBftNwFuA7YErgcfVnyVJkiSNKLZgR8SGwD9l5itXUzySJEnSVCu2YGfm3cDOEbHxaopHkiRJmmpd7sFeBPwwIuYBS2cGZuZHB4tKkiRJmlJdEuzf1K8NgC2GDUeSJEmabhMT7Mz8AEBEbF5/vmXooCRJkqRp1eWXHB8dEfOBBcCCiPhZRDxq+NAkSZKk6dOlm74TgLdn5s6ZuTPwDuBTw4YlSZIkTacuCfb9MvM7Mx8y87vA/QaLSJIkSZpinXoRiYj3ASfVn19F1bOIJEmSpBFdWrBfD8wBvgz8J7BdPUySJEnSiC69iCwBjlgNsUiSJElTr0svImdGxNaNz9tExBnDhiVJkiRNpy63iGyXmTfMfKhbtB8wXEiSJEnS9OqSYN8TETvNfIiInYEcLiRJkiRpenXpReQ9wA8i4ntAAE8FDhs0KkmSJGlKdXnI8RsRsRfwRKqW6yMz89rBI5MkSZKmUOstIhGxc0RsBVAn1EuBZwOviYiNV1N8kiRJ0lQp3YN9CvUvNkbE44AvAZcBewKfHD40SZIkafqUbhHZNDOvqt+/Cvh0Zn4kIjYAzhs+NEmSJGn6lFqwo/H+GcC3ATLznkEjkiRJkqZYqQX7rIg4Bbga2AY4CyAiHgzcuRpikyRJkqZOqQX7SODLwCXAUzLzrnr4g6i67psoIvaPiIsjYmFEHDVm/CYR8cV6/E8jYm5j3GMj4scRsSAiLoyI+3ZcJ0mSJGmNaW3BzswETh4zfH6XGUfEhsAngGcBVwBnR8S8zPxFY7I3AEsyc9eIOAQ4HnhZRGwEfB54dWaeHxHbAnchSZIkreW6/JLjbO0DLMzMRZl5J1WyfuDINAcCn63fnwo8MyKCqjvACzLzfIDMvC4z7x4wVkmSJKkXQybY2wOXNz5fUQ8bO01mLgNuBLYFHg5kRJwREedGxLsGjFOSJEnqTacEOyI2jYjdhw6mYSPgKcAr678vjohnjonrsIg4JyLOWbx48WoMT5IkSRpvYoIdES+g6vf6G/Xnx0XEvA7zvhLYsfF5h3rY2Gnq+663Aq6jau3+fmZem5m3AqcDe40uIDNPyMy9M3PvOXPmdAhJkiRJGlaXFuxjqO6nvgEgM88DdulQ7mxgt4jYpf5p9UOA0cR8HnBo/f4g4Kz64cozgMdExGZ14v004BdIkiRJa7lSP9gz7srMG6tnD++Vkwpl5rKIOJwqWd6Q6pcgF0TEscA5mTkPOBE4KSIWAtdTJeFk5pKI+ChVkp7A6Zl52sqsmCRJkrQmdEmwF0TEK4ANI2I34AjgR11mnpmnU93e0Rx2dOP97cDBLWU/T9VVnyRJkjQ1utwi8lbgUcAdwBeoevo4csigJEmSpGk1sQW7fsjwPXT89UZJkiRpfdalF5EzI2LrxudtIuKMYcOSJEmSplOXW0S2y8wbZj5k5hLgAcOFJEmSJE2vLgn2PRGx08yHiNiZDr2ISJIkSeujLr2IvAf4QUR8DwjgqcBhg0YlSZIkTakuDzl+IyL2Ap5YDzoyM68dNixJkiRpOnVpwQbYhOqHYDYC9ogIMvP7w4UlSZIkTaeJCXZEHA+8DFgA3FMPTsAEW5IkSRrRpQX7RcDumXnH0MFIkiRJ065LLyKLgPsMHYgkSZK0LujSgn0rcF5EfJvq59IByMwjBotKkiRJmlJdEux59UuSJEnSBF266fvs6ghEkiRJWhd06UVkN+DvgT2A+84Mz8yHDhiXJEmSNJW6POT4GeCfgWXA04HPAZ8fMihJkiRpWnVJsDfNzG8DkZmXZuYxwAHDhiVJkiRNpy4POd4RERsAv46Iw4Ergc2HDUuSJEmaTl1asN8GbAYcATweeBXwmiGDkiRJkqZVlwR7bmbekplXZObrMvOlwE5DByZJkiRNoy4J9rs7DpMkSZLWe633YEfEc4HnAdtHxP9pjNqSqkcRSZIkSSNKDzleBZwDvBD4WWP4zcBfDRmUJEmSNK1aE+zMPD8ifg48x19zlCRJkrop3oOdmXcDO0bExqspHkmSJGmqdekH+7fADyNiHrB0ZmBmfnSwqCRJkqQp1SXB/k392gDYYthwJEmSpOk2McHOzA8ARMTm9edbhg5KkiRJmlYT+8GOiEdHxHxgAbAgIn4WEY8aPjRJkiRp+nT5oZkTgLdn5s6ZuTPwDuBTw4YlSZIkTacuCfb9MvM7Mx8y87vA/QaLSJIkSZpiXR5yXBQR7wNOqj+/Clg0XEiSJEnS9OrSgv16YA7w5fo1px4mSZIkaUSXXkSWAEdExFbAPZl58/BhSZIkSdOpSy8iT4iIC4HzgQsj4vyIePzwoUmSJEnTp8s92CcCb87M/wcQEU8BPgM8dsjAJEmSpGnU5R7su2eSa4DM/AGwbLiQJEmSpOnVpQX7exHxL8B/AAm8DPhuROwFkJnnDhifJEmSNFW6JNh71n/fPzL8j6gS7mf0GpEkSZI0xbr0IvL01RGIJEmStC6YmGBHxNbAa4C5zekz84jhwpIkSZKmU5dbRE4HfgJcCNwzbDiSJEnSdOuSYN83M98+eCSSJEnSOqBLN30nRcSfR8SDI+L+M6/BI5MkSZKmUJcW7DuB/w28h6rXEOq/Dx0qKEmSJGladUmw3wHsmpnXDh2MJEmSNO263CKyELh16EAkSZKkdUGXFuylwHkR8R3gjpmBdtMnSZIkrahLgv3V+iVJkiRpgi6/5PjZ1RGIJEmStC5oTbAj4kL+0GvICjLzsYNEtJaae9Rpy32+5LgDOo2TJEnS+qXUgv381RaFJEmStI5oTbAz89LVGYgkSZK0LujSTZ8kSZKkjkywJUmSpB51SrAjYtOI2H3oYCRJkqRpNzHBjogXAOcB36g/Py4i5g0dmCRJkjSNurRgHwPsA9wAkJnnAbsMGJMkSZI0tbok2Hdl5o0jw1r7x5YkSZLWZ11+Kn1BRLwC2DAidgOOAH40bFiSJEnSdOrSgv1W4FHAHcAXgBuBI4cMSpIkSZpWxRbsiNgQOC0znw68Z/WEJEmSJE2vYgt2Zt4N3BMRW62meCRJkqSp1uUe7FuACyPiTGDpzMDMPGKwqCRJkqQp1SXB/nL9kiRJkjTBxAQ7Mz+7OgKRJEmS1gUTE+yI+C1j+r3OzIcOEpEkSZI0xbrcIrJ34/19gYOB+w8TjiRJkjTdJvaDnZnXNV5XZuY/AgeshtgkSZKkqdPlFpG9Gh83oGrR7tLyrdrco0679/0lx/ndRJIkaV3WJVH+SOP9MuC3wJ8NE44kSZI03bok2G/IzEXNARGxy0DxSJIkSVOtS4J9KrDXmGGP7z+c9Y+3j0iSJK1bWhPsiHgE8Chgq4h4SWPUllS9iUiSJEkaUepFZHfg+cDWwAsar72AP+8y84jYPyIujoiFEXHUmPGbRMQX6/E/jYi5I+N3iohbIuKd3VZHkiRJWrNaW7Az82vA1yLiSZn545WdcURsCHwCeBZwBXB2RMzLzF80JnsDsCQzd42IQ4DjgZc1xn8U+PrKLluSJElaU7rcgz0/It5CdbvIvbeGZObrJ5TbB1g484BkRJwMHAg0E+wDgWPq96cCH4+IyMyMiBdR9ViytMuKSJIkSWuDiT80A5wEPAh4DvA9YAfg5g7ltgcub3y+oh42dprMXAbcCGwbEZsDfwN8oMNyJEmSpLVGlwR718x8H7A0Mz9L9SuO+w4bFscAH8vMW0oTRcRhEXFORJyzePHigUOSJEmSJutyi8hd9d8bIuLRwO+AB3QodyWwY+PzDvWwcdNcEREbAVsB11El8AdFxIepHrK8JyJuz8yPNwtn5gnACQB77713dohJkiRJGlSXBPuEiNgGeB8wD9gcOLpDubOB3eofpbkSOAR4xcg084BDgR8DBwFnZWYCT52ZICKOAW4ZTa4lSZKktdHEBDsz/7V++z3goV1nnJnLIuJw4AxgQ+DTmbkgIo4FzsnMecCJwEkRsRC4nioJlyRJkqbWxAQ7Ih4IfAh4SGY+NyL2AJ6UmSdOKpuZpwOnjww7uvH+duDgCfM4ZtJyJEmSpLVFl4cc/42qFfoh9edfAUcOFZAkSZI0zbok2Ntl5inAPXBvd3p3DxqVJEmSNKW6JNhLI2JbIAEi4olU/VVLkiRJGtGlF5G3U/X28bCI+CEwh6rHD0mSJEkjWhPsiNgpMy/LzHMj4mnA7kAAF2fmXW3lJEmSpPVZ6RaRrzbefzEzF2Tmz02uJUmSpHalBDsa7zv3fy1JkiStz0oJdra8lyRJktSi9JDjnhFxE1VL9qb1e+rPmZlbDh6dJEmSNGVaE+zM3HB1BiJJkiStC7r0gy1JkiSpIxNsSZIkqUcm2JIkSVKPTLAlSZKkHplgS5IkST0ywZYkSZJ6ZIItSZIk9cgEW5IkSeqRCbYkSZLUIxNsSZIkqUcm2JIkSVKPTLAlSZKkHplgS5IkST0ywZYkSZJ6ZIItSZIk9cgEW5IkSeqRCbYkSZLUIxNsSZIkqUcm2JIkSVKPTLAlSZKkHplgS5IkST0ywZYkSZJ6ZIItSZIk9cgEW5IkSeqRCbYkSZLUIxNsSZIkqUcbrekA1G7uUact9/mS4w5YQ5FIkiSpK1uwJUmSpB7Zgj2lbN2WJElaO9mCLUmSJPXIBFuSJEnqkQm2JEmS1CMTbEmSJKlHPuS4jmo+BOkDkJIkSauPLdiSJElSj0ywJUmSpB6ZYEuSJEk9MsGWJEmSemSCLUmSJPXIBFuSJEnqkQm2JEmS1CMTbEmSJKlHJtiSJElSj0ywJUmSpB6ZYEuSJEk9MsGWJEmSemSCLUmSJPXIBFuSJEnqkQm2JEmS1CMTbEmSJKlHJtiSJElSj0ywJUmSpB6ZYEuSJEk9MsGWJEmSemSCLUmSJPXIBFuSJEnqkQm2JEmS1CMTbEmSJKlHJtiSJElSj0ywJUmSpB6ZYEuSJEk9MsGWJEmSerTRmg5Aq9/co05b7vMlxx3QaZwkSZImM8FWZybfkiRJk5lgqzfNBNzkW5Ikra8GvQc7IvaPiIsjYmFEHDVm/CYR8cV6/E8jYm49/FkR8bOIuLD++4wh45QkSZL6MliCHREbAp8AngvsAbw8IvYYmewNwJLM3BX4GHB8Pfxa4AWZ+RjgUOCkoeKUJEmS+jRkC/Y+wMLMXJSZdwInAweOTHMg8Nn6/anAMyMiMnN+Zl5VD18AbBoRmwwYqyRJktSLIe/B3h64vPH5CmDftmkyc1lE3AhsS9WCPeOlwLmZeceAsWpgPiApSZLWF2v1Q44R8Siq20ae3TL+MOAwgJ122mk1RiZJkiSNN+QtIlcCOzY+71APGztNRGwEbAVcV3/eAfgK8JrM/M24BWTmCZm5d2buPWfOnJ7DlyRJklbekAn22cBuEbFLRGwMHALMG5lmHtVDjAAHAWdlZkbE1sBpwFGZ+cMBY5QkSZJ6NViCnZnLgMOBM4CLgFMyc0FEHBsRL6wnOxHYNiIWAm8HZrryOxzYFTg6Is6rXw8YKlZJkiSpL4Peg52ZpwOnjww7uvH+duDgMeU+CHxwyNgkSZKkIQz6QzOSJEnS+mat7kVE6we78JMkSesSW7AlSZKkHplgS5IkST0ywZYkSZJ6ZIItSZIk9cgEW5IkSeqRCbYkSZLUI7vp01qv2Y2fXfhJkqS1nS3YkiRJUo9MsCVJkqQemWBLkiRJPfIebE01f2ZdkiStbWzBliRJknpkgi1JkiT1yFtEtM7y9hFJkrQm2IItSZIk9cgEW5IkSeqRCbYkSZLUIxNsSZIkqUc+5Kj10qQHIJvjfThSkiStDFuwJUmSpB6ZYEuSJEk9MsGWJEmSemSCLUmSJPXIBFuSJEnqkb2ISCvJn2CXJEklJthSj0y+JUmSt4hIkiRJPbIFW1qN/AEbSZLWfbZgS5IkST2yBVtaS9i6LUnSusEWbEmSJKlHJtiSJElSj0ywJUmSpB55D7Y0BexfW5Kk6WELtiRJktQjE2xJkiSpRybYkiRJUo9MsCVJkqQe+ZCjNOUmPQDpD9hIkrR6mWBL6zF7J5EkqX8m2JLGMvmWJGl2vAdbkiRJ6pEJtiRJktQjE2xJkiSpRybYkiRJUo98yFHSSrNrQEmS2plgS1qt7J1EkrSuM8GWtNawZVyStC7wHmxJkiSpRybYkiRJUo+8RUTSOqF0e4n3fUuSVicTbEnrNe/7liT1zQRbkmbJVnNJ0jgm2JK0BtgyLknrLh9ylCRJknpkC7YkrWVs3Zak6WaCLUlTxHu7JWnt5y0ikiRJUo9MsCVJkqQeeYuIJK0jvH1EktYOJtiStJ4oPTxpn96S1B8TbEnSKumauJvUS1pfmGBLkqaOXRlKWpuZYEuS1imzbRmf1GpuUi+pKxNsSZJWkbe7SGoywZYkaQ2a7T3sktZeJtiSJE2hoW53scVdWnX+0IwkSZLUI1uwJYoVEq8AABaDSURBVElSJ321jNtlo9Z1JtiSJGlqebuL1kbeIiJJkiT1yBZsSZK03vG2FA1p0BbsiNg/Ii6OiIURcdSY8ZtExBfr8T+NiLmNce+uh18cEc8ZMk5JkiSpL4Ml2BGxIfAJ4LnAHsDLI2KPkcneACzJzF2BjwHH12X3AA4BHgXsD3yynp8kSZK0VhvyFpF9gIWZuQggIk4GDgR+0ZjmQOCY+v2pwMcjIurhJ2fmHcBvI2JhPb8fDxivJEnSRPaIokmGTLC3By5vfL4C2LdtmsxcFhE3AtvWw38yUnb74UKVJElas9ZEN4drer7ras8vkZnDzDjiIGD/zHxj/fnVwL6ZeXhjmp/X01xRf/4NVRJ+DPCTzPx8PfxE4OuZeerIMg4DDqs/7g5c3Bi9HXBtS3ilcatS1mUON1/XZfqWOdR8XZfpW+ZQ83Vdpm+ZQ83XdVk757suL3PnzJzTupTMHOQFPAk4o/H53cC7R6Y5A3hS/X6jOvAYnbY53Uos/5zZjFuVsi7TdXGZrovLdF1cpusybctcl9ZlTW2/0deQvYicDewWEbtExMZUDy3OG5lmHnBo/f4g4Kys1mIecEjdy8guwG7A/wwYqyRJktSLwe7Bzuqe6sOpWp83BD6dmQsi4liqbwHzgBOBk+qHGK+nSsKppzuF6oHIZcBbMvPuoWKVJEmS+jLoD81k5unA6SPDjm68vx04uKXs3wF/twqLP2GW41alrMscbr6uy/Qtc6j5ui7Tt8yh5uu6TN8yh5qv67J2znd9WeYKBnvIUZIkSVofDfpLjpIkSdL6xgRbkiRJ6tGg92Br3RARDwVeAuwI3A38CvhCZt60RgNbD0TEI6h+ZOmnmXlLY/jzgfsDV2XmtyLiFcAfAxcBJ2TmXWsk4I4i4nOZ+ZqBl7EvcFFm3hQRmwJHAXtRPTz9ocy8ccjlr6yI2AfIzDw7IvYA9gd+WT/LolVUH0sH8ocfLbsSmJeZFw20vCOAr2Tm5RMnHkhEPIXqV5B/npnfXFNxSOuj9fIe7IjYNjOvW9NxTIP6IvF84PvA84D5wA3Ai4E3Z+Z3e1zWAzLz96ur3JrStf7V2/4tVEnz44C3ZebX6nHXA2cCm1Htj82BLwPPpDquDx070zUgIka75wzg6cBZAJn5wlWY9+sy8zMt4xYAe9Y9Gp0A3AqcSrWN9szMl8x2uSsZ48T9HRHvB55L1ehxJtUPbn0HeBbV7wmsygPfK2XajqcuIuJvgJcDJ1P9MjDADlQ9V52cmccNsMwbgaXAb4D/AL6UmYv7Xs7IMv8nM/ep3/851fnjK8Czgf8aYj3Vbra5xrp4DK6XVqbT7LXxBTy28f4+wHup+tH+EFXycRywXT1+b2ARsBC4lOqk8y7gr4H7Aq+ty36YKmHZm+oi93mq1tszgRup+vj+ozGx/Kr+u39j2FZU3RFeAHwBOA14FbB5y/psVcf8S6quC6+jSrCOA3YqjNu6ZX4PoOom8S+AvwWePDL+I8CngQ/W6/wp4OfAl4C5wIXAhvW0mwHfrd/vVI/7Z+ATVD9xf0w97BSq3mNK63n/kde2wCXANvU+mdlnu1Il9zcAPwWeXCh3f2BL4O+Bk4BXjCxzcV0/HjYmnnPbxnWog18vjDuvUP/2A15f14nz6xhOBvarp79wZvvV++IcqiQb4Lb670bANY19FMAFE+LdHDgWWEBVnxcDP6Gq/5vRfkzsWzrWCsu7luoY2g94Wv336vr901bx+L9szLBt678XNffv6H4pzLN0DG49oY6dW9jfzy2Uu47qON0MuAnYsh6+KdW5o7TPDqf9eHliYZmfpHwc3r+wjb5M+7nxlYVxf0T5/LhrYdtvS3Ue+0Y9/QXA14E3AfeZUE9+NW4aYGPg16tQ/0rrciHVbZjProcvrmM/tJ627Zz8PsrnhdZzFTC/8f5sYE79/n71dmw7Xz94wnqeXVjPB85y27WeNyetZ9s+rv8+qLCeO5TqEPBQ2q+HL5hQr0u5xv60H7+lY/ApjfUbl9+U4n0E7efyh9B+nN2f9rr5XqprTts23KGw7R9CuV5/mULOUNjvC2g//z2mrZ7U71vPq6tSd++dbrYnlrXlRePCSZUs/hvVhftjwOeACxvjvwM8oX7/8LpifYTqQvNt4OPAU4H/TXVB+h+qi+LLgcuBg+qyz6S6VeKm+nVz/bp75m9jmf9aV/6dgb8CbqNqRbu+rnQvBjZuTH8G8DfAgxrDHlQPu7Yw7pu0H6ifq5d5JPAz4KON8jcDf0n17/OfA++gOnm8gaqF8UJgk3rabWj8klFd9q112QvqOHash01az3uA34687qr/3tmY7jTgxfX7/YAslFsE/CfVSeJFVCeT/2zEfwfwD8Bl9b79K+Ah9bjfto2rx+/V8no88Lsx235m+99VqH/XUp2AngL8I9WB/izgW/U2XDBS1zenOql9tN6+G9f75GbqZIjqRHoR5STxNKqT7Q7A26ku6LsBn62naTsmriscaycXtsEV9fY8E3hcXX5RY16lhPWT/OEkPvq6kKqf/LaL2neA19XjPgPs3dj2C2i/WP6I8nFWqmO3Ffb3kkK5Wxvl5o9sg/OArxX22eLC8XJtYZnnUj4OLyns0ztpPzfeXBj3Y5Y/Z4+eH68pbPurqS7eT6y3ww71+38Gvkj5S8gvqX7WePT6sTNwceH68nXKCdsFhXW5YWRe9wFeSNWafRtVcjrunHwd5fNC67mKKnHZpo7znJHl30T7+fprlM9xdxXW80eNcaPJ958W5nk15S8LpfW8mfbr77LCel5BuQ59n/br4U2U63Up11hC+/FbuqbdMSG/KcV7De3n8qtoP86uoL1unktVf9u24dWFbX8Z5Xp9JS05A93r5uj5r5Sn3UT5vHpiYZlXd8pPu0y0Nr9Y/hv7edStFNQteVQJw0b1sJ+MlL2tMe3v+MMtMzNlm/O+bKTs7+sK/sDGsN/OVMJmTCPlbq3/bgm8mqqldzFVAvBsyif7OwvjLqb9YnkndUJD9e3zBKpvi5uw/IV9dB3nA2+rt8WnqC5SMwnLHOCWQtlJ6/kOqmTxMY0yM9vv4saws0fme1VbuZbt/R7gh1QXnOa6PpXqxPM7qpPhpYVxh1EdlGfVn0dfSZXYNbf7zOecVP8an39S/92Eqt6eRZ2QNqbZqK5399TLuBQ4guoE+imqC//7KX9Ru3lknmfXfzcAbi8cE83kcfRYK22DO+vpdqBqWfl4s75QTljPpbpQPI7qgt58zaX8BWY+1QXpN1StGXfVMX2v3k5tF8vl9suY46xUx25bif3dLLeU+r8AwAaNabaqt8H5hX12x+jwjss8l/JxeHdhn94z6dgvnFNK58fbZ3n++xXli+XJVF+8vk51/juhXu+FVLdRlJLAb9CeNNxQWJdbC/E2k7HRc/Kk80Jz+42eq65t7KNF1C3TVF8+SvvlPMrnuHu6rCcrJt9ZmOdtlJPo0np+m/brb+m6Xapfv5pQdlK9nphrtBy/19B+DE7KbybGy/hzeWk73N54P1o359NoAZ5QdjSeSfV6fv15XM7QtW6Onv+ubasn9fvSebVYd9u2wXLz6zLR2vyiOom8GHgpjX8Jz2w8qpPgN4FnUH17+ieqb4AfAK5vTPvpMWV/TJUMHkyVyLyoHvc0qn/XP77eAUfUO2Qmib2C6gT/jjq+aKtk9bBtqf69clYd67tGKsQDqU7q1xXGfYuWiyXVg1Kjy3w/1YX2dqpk5Al1ZZxp5duVunUGeBTVT9k/YnQbNd5/sHQwja5n/Xkm4foosEVj+/0dVWL0UOB/UX2T3hl4HfDfbeXqshfRSFDqYa+latVa4QJN9W/5/Wm0zo4Z9xmq1oHdWurgXcBOLeOWFOrfddQtN1QX9O83yv2iXs8Htcz3yVT/cpu5GG1d76N96s+lL2q3Uf/rkapF7YzGuOYJcvSYuIPqYddxx9odhW1w+cjnA6geMpz5XEpYz6VqSXhKy7xvpP2idmH9d0tgT6rj9YH1sNKF6SbKx1mpjl1f2N83Fspd2rJ+2wGPoWpVb9tn19F+vNw8aZm0H4e/LuzTO2g/N95SGHcO5fPjzYVtf2M9z+YXkA2Al1F9gSpdLH9Z/30iVf19af1+Q8oX79sm1JU7C+uywnm3NI7lz8ml88L8MWXvPVe1LG8zlr9lavR8fQHlc9yywnou98V7zLmmbZ6XU06iVzgmWP6c3Hb9LV2Xlk6oQz+juh7uw4rXw6WU63Up1/g97cfvxbQfg4toP+eeX4h3t5H9MnouLx1nSwt189dU/xVq24ZLC9v+Vsr1+twxy53JGW4p1KObKOcLY+tJXbZ0Xr2jsMzLxw1fYbouE63Nr3rDfqbxmrmAPgj4dv1+P6p//8ynark6napV8kTG3O8DPAz4AdVF+QyqVo9H1AfMEqqL05MbFesI4P9R9egwUxmbrzmNmK6ZsD7bAMdTXRCWUF2wL6qH7VIYN3OLwAoHKtW/wfcfs6w3Up04L67n8xSq1sNfU50QXjQh1mNbtt+uNP5t3WEfvpDqoP1dY9jrqE5411KdDH5Bdd/ZVhPKfRj40zHL2J+RltuR8SdPiPEgYPeWcf9C9dDcuHFvLdS/Z1G13Cyk+iL0xLrMHODDq3hclL6o/YSqtWhJXc93byz3R4Vj4hraj7WLS9tgQqylhHVs4jmyfdsuaicVypW+PM+ncJxNqGO/LuzvfyiVm7Cee47ss4c39tkR9fYad7z8Y9dlMnI8UbXstu3T42g/Nx46ZtwN9bg/pnx+PLmw7fest+vvqVobf1W//yLVubF0sSx94SwllpdTTtiuKazL5wrLnHROvqyuS7+lfvah3tcfZsK5qrDM0vn6VMrnuP8orOdS2pPvSwvzfBHjk6qZJHpRh3Uad/0trefpdX1ZXNefmWvdTB16Ju3Xw7eV6nW9jKcz/tjfi+r4vYExx2/hGPwM1T3WY/ObCfF+s2U7PIzq/Hd8Xe56lj/OTinUzbuo/nPYdhz+n8K2/x7lev39wn4u1c0X0X7+26qtntTDH0v7efXE0jI7HXOzOVDX9heNExvV0/gzDwxtRnXw/XddkbZqK0v1b5R9Gzto01JZ4MEsf4/qI+rKv/nIdPtTfduc+Vf2HlQnp+eNlP3TMWXf1oinuC6seKAWlzlS9r8ZSXgK27p1PUv7ZbRsvX0fXSh7Usv+/DBVq+K926AQ05Es//DYscB/zZSdtC6zXdcJdSGo7yEet41W4RhoflEbPYFuAzyypY611c8D+MMxMbYerUwdG1lmMWHtUH4/xl/UNiqUGfflebmLZWPap1IlD89umddT6nVdYXyzLOPPJ/fWv5Xcv8stc2TbP6pe5vOoLio7rsR8n0p1T+yzqf51+5qZfQO8gur2nrcw8sDg6Daq61fXY+XedZkUb70N96Fq2Xoy8M5G/StdLI8ozHPSxbuUsJ3Fiufk2e7T5jnjSW3H0sru0651aLb7hfIXptMpn3M7f1mYUOefChxd1/l9R5b5gXH7pK5D2wKfH1PHmsfSO+lwLE3YRp3Hsfy1cBOqL6xjj0Gq53Ca419J9V+At1A92Np6/FIl2n9NlRR/jKqleEvKx/3GjWU+q95+zWUeWVjPjakazg5uiae03EnzbZZ7JdXzEm+hOiab415N9SXwzfUym+fkzUbrClWr+Duprg8fndlGXevs1HfTN6YLMKhas86q3z+M5bvqWkr1Le+ZVBvr7ObsaHQftpJlm8vdhWqnj+te7Sqqb3Fju+Oi+vbV1jXbbcAWLfGs0O1Y3ffvw6j+vfQaqgvP6DLnUH2jHLv9stCFWkS8laoHg3Gx3kD1AEbbtv1WYT1HyzZj2o+qtX7sNqjXqy2m0e3X7LbtxVQntnHlzqX6T0nXeJvrWqoLpfUsbvtVERH/TvVvs1+OialUP++k+jI0M24f4Lst43rpZq7UDd9QZSNiUWY+tH7/Rqr9/lXqrs6Al2R7V2g7ZeaOLWV3BbZvqX/FbgNHul97I1Udn1nmrVT/rRq3X/ah+nI1tqu4Md26vbkR72ZUrVPjuoI8IDPntJS7jurf1uPq17nAssK6PL4R7xfqeK+tp30/y3dl2FzPYh1bhbpQLBcRV1I9PLlS+3RC15UPp9reY9eT6j9TK93936Qu/CjX6+Z+WZllzmr71GWbx+FoPKU6vzvtx1npug3VLRdtdWzSsdTslnG07o6OO3Wm7JjuHE9plPv3OpZxxyBUSeJGVNeuG6mS6q/U4/+kXtdxZR9ZTz+u+90r6/WczTJfQdWKvMI2Wol1GTd+ZeZb2gbNcUH1YHxbV64vofoP1ey7KO6aia+tr3qlW7sAo9xV160DlV1EoXs1yt1xlbpmu70QT6nbsQupkqZxy7yttA0mbPtSrJO27WzLXlraBquw/W4rlJs/Yb6zrQvFbTTgMXNnIaZS/ZztuGK3gRNiXaEbvqHLsnwvNqNdnV1IuSu02wrjZnX8ztTB0nwn7Je2ruK26LIujO8Ksriehfo1f9IyC/EumG0dW4W6UCzH8g/Zrsw5uXTdWlRaz7ps6z6dZR2aWK9nucxZbZ8Ox2GpzpeOsy7XpVkfS4Xx589y3M8Lx+AF/OE5qdZjtG0c7d3vFruAnbTModZllvOdtC6lHK+0jVZ4DmJsHZ7NSWdtetUbvdQF2Jdo76rr7CHKUu5ebVJ3XKWyN0yIp607s9uoexpoWWbrNpiw7UuxFuc727Id9slst9/oE86j8cw23llvo1U8Lkpd291TiKlUP2c7btKFtBTrHUOUnbR9aO/qbD7lrtBuK4xbUqq7E9aztMxO+6X+3OwqbnGHdWnrCvL2UrkJdb7TuoyJ967Cep43UF24Y8L4u2ezTylct1g+0R23nqOJwHL7dJZ1aFK9Lu2XZbPdPhO2bek4LNX5SdfJ0nWptO0nHUul/XLXLMcto9wd688L42+fMK6t+91SuUnLHD32+1qX2c530rqU8olbC9vo523H2XJxdploGl60dwG2Fe1dde05RFnK3asl5e64SmXvLsVDe3dm86n7bRy3zNI2mLDNi7FO2LazKjtpn6zC9vufCeVmG++st9EqHg+lru3uKMRUqp+lruSK3cytQqxXDVF2QrlltHd1dh5V/9Bt4+8sjLugUP/GPkzYiLe0zNK2L3UVt9mE+V5Je1eQSwrlbi7Ur7snLLMU79mlOjZQXbhqwvirZ7tPC+eMn05Yz9bWs5lys6hDk+p1ab/MevtMKFs6Dkt1/rxJ+2Tcdu+w7ScdS6X9UvpvRmncuyh3x/pXhfFfL4w7nfbud3+zCsts7R96FddltvMtbYP3U84nji9so9YHMpeLrctE0/RipAuwxvAVuuoaqizl7tX2axk+0x1XsWu2Ujy0dGdG9fDAF9qW2WUbtMQzMda2+a5K2QnbYFbbb1K52ca7quu5CsdBqWu7r8yyfu41y3Er/JrWSsS6Qr3to+xsylFdRHcpzLN1fHNcW92d5X7ejPYH9LYDnr8K892FQleQhXJP6lrnx5R9RmH8JqU6NlRd6DLfVd2nI+eMSev58FWpMytTbzvul1lvn1U4Dkt1/jFd9wkrXpdK2754LJX2y2zH1eOLx2Bp/IRxY7vfXZVlDrUuqzLfScucUD9bt1GX19Q/5ChJkiStTTZY0wFIkiRJ6xITbEmSJKlHJtiSJElSj0ywJUmSpB6ZYEuSJEk9+v8BLN+vhRnqwqAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOHUHBTI-GYU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae351345-e443-4cfc-809d-795417f710b9"
      },
      "source": [
        "pred1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 2, 2, ..., 1, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH3db3uvnQiC"
      },
      "source": [
        "final_val = np.array(pd.DataFrame(val).idxmax(axis=1))\n",
        "final_open_test = np.array(pd.DataFrame(open_test).idxmax(axis=1))\n",
        "final_open_test_b = np.array(pd.DataFrame(open_testa).idxmax(axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwzgYMCAIsXV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "ebbd50ea-3cca-4d7b-84ce-3f0f221eabd2"
      },
      "source": [
        "print(show_evaluation(val1, valid1_lithology))\n",
        "print(show_evaluation(open_test1, valid2_lithology))\n",
        "print(show_evaluation(open_test11, valid3_lithology))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default score: -0.617504098037325\n",
            "Accuracy is: 0.7683359776996642\n",
            "F1 is: 0.8038841337036852\n",
            "None\n",
            "Default score: -0.5671663004143571\n",
            "Accuracy is: 0.7890605354936753\n",
            "F1 is: 0.8165453402739566\n",
            "None\n",
            "Default score: -0.5951040156461327\n",
            "Accuracy is: 0.7775583081159448\n",
            "F1 is: 0.8089888016227615\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5si6wyqJz3Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "78232d5e-40f0-453f-d43f-d3b05b9df03f"
      },
      "source": [
        " print(show_evaluation(val2, valid1_lithology))\n",
        "print(show_evaluation(open_test2, valid2_lithology))\n",
        "print(show_evaluation(open_test22, valid3_lithology))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default score: -0.6234298161800156\n",
            "Accuracy is: 0.7660217919161283\n",
            "F1 is: 0.7986957722089358\n",
            "None\n",
            "Default score: -0.5648157258902118\n",
            "Accuracy is: 0.7891917303508369\n",
            "F1 is: 0.8158804624594801\n",
            "None\n",
            "Default score: -0.597346822609051\n",
            "Accuracy is: 0.7763323051774299\n",
            "F1 is: 0.8058267796854165\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}